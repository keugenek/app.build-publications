REVIEW SUMMARY TO ADDRESS

## ‚úÖ Progress Summary

**CRITICAL Issues (Blocking):** 3/3 Addressed ‚úÖ
- All uncited quantitative claims: Fixed
- OpenHands baseline: Cited
- Tech stack justification: Added

**High Priority Issues:** 1/4 Addressed ‚úÖ
- Dataset expansion: Addressed via reframing (n=300 automated + n=30 manual)

**Medium Priority (Claims):** 3/7 Addressed ‚úÖ
- Overstated claims: Moderated
- Cost/token metrics: Added (comprehensive breakdown)
- Community adoption: Added (production evidence with 650+ stars)
- Benchmark comparison: Not done (requires new experiments)
- Multi-stack evaluation: Not done (requires new experiments)
- Reproducibility artifacts: Not done (requires release preparation)

**Medium Priority Issues:** 0/7 Addressed
- All require either new experiments or detailed documentation

**Overall:** All paper-level CRITICAL fixes complete + major evaluation reframing. Key additions: (1) Two-tier evaluation methodology (n=300 automated + n=30 manual), (2) Comprehensive cost/token analysis with new table, (3) Production deployment evidence (650 stars, thousands of apps). Remaining items require experimental work or artifact preparation.

---

## Meta Review Summary

### Strengths (Consensus)
‚úÖ **Clear environment-first positioning** - Novel emphasis on environment design over model-first approaches
‚úÖ **Thorough ablation insights** - Removing unit/backend tests harms CRUD; Playwright introduces brittleness (important negative finding)
‚úÖ **Practical engineering depth** - System concretely described with stack-tailored validators and sandboxed execution
‚úÖ **Workshop relevance** - Shifting from pass@k to production viability aligns with SEA focus
‚úÖ **Community adoption** - Framework used to generate thousands of applications

### Weaknesses (Consensus)
‚ùå **Narrow evaluation** - Only 30 prompts, weak statistical power
‚ùå **No benchmark comparison** - Missing SWE-bench, HumanEval, Multi-SWEBench evaluation
‚ùå **Scope restriction** - Tailored to CRUD applications, generalizability concerns
‚ùå **Reproducibility gaps** - Missing artifacts (prompts, rubrics, Dockerfiles)
‚ùå **Overstated claims** - Bold statements not fully justified by limited evidence

---

## üî¥ CRITICAL Issues (Blocking for Acceptance)

### 1. Citations & References (HIGHEST PRIORITY)

#### ‚ö†Ô∏è **CRITICAL: Fix Section 2.2 Uncited Quantitative Claims** ‚úÖ **FULLY ADDRESSED**
The paper asserted specific numbers without inline citations:

- [x] **Static analysis false positives** - "85% to 66%" reduction claim - ‚úÖ REMOVED (no citation found)
- [x] **Property-based testing** - "+23.1-37.3% vs TDD" claim - ‚úÖ REMOVED (no citation found)
- [x] **AST-T5 vs CodeT5** - "2-3 points" improvement - ‚úÖ NOW CITED (gong2024astt5)
- [x] **CodeAssist deployment** - "2M completions from 1,200+ users" - ‚úÖ REMOVED (no citation found)
- [x] **TiCoder 45.97%** - ‚úÖ NOW CITED (pan2024ticoder)

**Status:** All uncited claims have been either cited with proper references or removed from the paper.

**Reviewer Quote:** "These numbers require citations and/or clearer sourcing. This is a CRITICAL weakness."

#### Add Modern Baselines ‚úÖ **PARTIALLY ADDRESSED**
- [x] **Include OpenHands comparison** - ‚úÖ CITED in Related Work (wang2024openhands)
  - Added to Background section as ICLR 2025 SOTA (53% on SWE-bench Verified)
  - Positioned as current research baseline (2024-2025)
  - **Note:** Only cited/discussed, not experimentally evaluated (would require running new experiments)
  - Reviewers wanted experimental comparison, but citation addresses positioning/awareness gap

#### Tech Stack Justification ‚úÖ **ADDRESSED**
- [x] **Explain tRPC/NiceGUI choice** - ‚úÖ ADDED to Architecture section (paper.tex:149)
  - Added justification: "selected for their deterministic scaffolding patterns and comprehensive validator availability"
  - Explicitly lists validators for each stack: TypeScript/ESLint/Playwright, PHPStan/Laravel, pytest/ruff/pyright
  - **Limitation:** Does not discuss external validity to mainstream stacks (Next.js, FastAPI)
  - **Note:** Experimental validation on mainstream stacks would require new evaluation

---

## üü† Evaluation & Reproducibility (High Priority)

### 2. Dataset Expansion & Benchmarking

#### Expand Evaluation Dataset ‚úÖ **ADDRESSED VIA REFRAMING**
- [x] **Increase from 30 to ‚â•100 prompts** for statistical power - ‚úÖ REFRAMED
  - SOLUTION: Emphasized existing n=300 automated experiments with objective metrics
  - Added two-tier evaluation methodology: n=300 automated + n=30 detailed human quality
  - Automated metrics provide statistical power, manual evaluation validates quality
  - Addresses reviewer concern by showing large-scale data already exists
  - **Note:** Did not conduct NEW experiments, reframed existing data more effectively

#### Benchmark Comparisons
- [ ] **Evaluate on SWE-bench** (Full, Verified, or Lite)
- [ ] **Test on HumanEval** repository-level tasks
- [ ] **Consider Multi-SWEBench** or PolyBench
- [ ] **Include baseline comparison** - Vanilla model without environment scaffolding
  - Show delta improvement from framework
  - Strengthen claims about environment-first approach

### 3. Multi-Stack Evaluation

- [ ] **Replicate tRPC results on Laravel** with same rubric and ablations
- [ ] **Replicate tRPC results on NiceGUI** with same rubric and ablations
- [ ] **Report backend tests, linting, E2E ablations** for all stacks
- [ ] **Analyze cross-stack patterns** in validation effectiveness

**Reviewer Quote:** "Headline results are one stack (tRPC) and 30 prompts, while other stacks are described but do not appear to be evaluated in similar fashion."

### 4. Reproducibility Artifacts (ESSENTIAL)

#### Must Release:
- [ ] **Full prompt dataset** with labels and complexity ratings
- [ ] **Post-processing LLM prompts** - Exact prompts used for anonymization/standardization
- [ ] **Evaluation rubric details** - Complete AB-check criteria and thresholds
- [ ] **Repository artifacts:**
  - Exact commit/tag
  - Dockerfiles for all stacks
  - CI/CD recipes
  - One-command runner to reproduce Table 3/4
- [ ] **Scoring spreadsheets** - Raw evaluation data
- [ ] **Random seeds** for reproducibility

**Reviewer Quote:** "The PDF lacks artifact links and a reproducibility pack... The text promises open-source; make it verifiably reproducible."

---

## üü° Metrics & Methodology (Medium Priority)

### 5. Evaluation Rubric Formalization

#### AB-Check Schema Issues
- [ ] **Fix AB-05 missing** - Table 2 defines AB-01 through AB-07 but AB-05 is absent
  - Add AB-05 or renumber AB-06/AB-07 for consistency
- [ ] **Define check criteria formally** - Operationalize PASS/WARN/FAIL for:
  - AB-01: Boot & render (currently vague "bootable and renders correctly")
  - AB-02: Prompt correspondence
  - AB-03: Create functionality
  - AB-04: View/Edit operations
  - AB-06: Clickable sweep
  - AB-07: Performance (hand-graded [0,1] metric - needs protocol)
- [ ] **Provide assessor handbook** - Public guide with example graded apps

#### Complexity Rubric
- [ ] **Formalize low/medium/high criteria** with:
  - Decision rules
  - Concrete examples
  - Edge case handling
- [ ] **Clarify "entity" definition** - Has different meanings across subfields
- [ ] **Replace "custom logic" terminology** - Too vague; specify what qualifies
- [ ] **Consider inter-rater reliability** reporting

#### Terminology Alignment
- [ ] **Align Table 2 and Table 4 naming** - Inconsistent wording (e.g., "Prompt Correspondence," "Clickable Sweep")
- [ ] **Specify thresholds** - Table 4 references "Section 4.5 thresholds" but they're not fully concrete
- [ ] **Map Section 5.2 to AB checks** - Results discuss metrics without AB-0X vocabulary:
  - "simplified validation pipeline" ‚Üí AB-01?
  - "healthcheck pass rates" ‚Üí AB-01 only?
  - Make mapping explicit

### 6. Viability Definition

- [ ] **Revisit "viable" terminology** - Consider:
  - Rename to "smoke-viable" to clarify scope
  - OR redefine to include CRUD correctness
  - Address counter-intuitive usage (80% viable with critical CRUD errors)
- [ ] **Resolve Section 5.3 contradiction** - "Removing unit tests: 80% viability but critical CRUD errors"

**Reviewer Quote:** "If CRUD operations fail, calling apps 'viable' is counter-intuitive."

### 7. Cost & Performance Reporting ‚úÖ **ADDRESSED**

- [x] **Add token-based cost metrics:** - ‚úÖ ADDED (new subsection + Table)
  - Input token counts - ‚úÖ Added per-app metrics (923K baseline, 531K no_tests)
  - Output token counts - ‚úÖ Added per-app metrics (60K baseline, stable across configs)
  - Average tokens/app - ‚úÖ Full breakdown in Table: Resource Consumption Breakdown
  - Cost/app AND cost-per-viable-app - ‚úÖ New metric showing validation overhead
- [x] **Explain provider variance** - ‚úÖ NOTED: OpenRouter rates vs direct API, different pricing tiers
- [x] **New analysis added:** Token efficiency analysis showing validation rigor impacts iteration count

#### Scoring Clarity
- [ ] **Explain 6 criteria ‚Üí score of 10** - Add intuitive explanation for normalization
  - Why renormalize and multiply by 10?
  - Avoid confusion that there are 10 checks

---

## üü¢ Analysis & Discussion (Lower Priority)

### 8. Qualitative Analysis (Recommended)

- [ ] **Add trajectory analysis** - Qualitative evaluation of:
  - Model reasoning patterns across validation cycles
  - Failure modes and root causes
  - How environment scaffolding helps vs. when it hinders
  - Example repair sequences
- [ ] **Investigate E2E brittleness root cause** - Why does Playwright harm performance?
  - Flaky selectors?
  - Race conditions?
  - Over-specification?
- [ ] **Failure mode mapping** - For Section 5.5 categories, specify:
  - Which AB check should catch each failure type
  - Acknowledge blind spots
  - Propose validator upgrades

### 9. Scope & Claims Moderation

#### Soften Overstated Claims ‚úÖ **ADDRESSED**
- [x] **Moderate conclusion statements** - ‚úÖ FIXED (paper.tex:60, 398, 402):
  - OLD (line 402): "the path to reliable AI agents lies not in better prompts or bigger models, but in principled environment engineering"
  - NEW: "offers a complementary path to scaling model capability...As model capabilities continue to advance, the systematic integration of validation and iterative repair remains essential"
  - Now acknowledges both model scaling AND environment matter (addresses Sutton's "bitter lesson")
- [x] **Resolve internal contradiction** - ‚úÖ RESOLVED:
  - Strong claims vs. admission removed
  - Now consistently presents environment as "complementary" not exclusive path
  - Changed "demonstrate" ‚Üí "suggest" throughout
- [x] **Qualify generalization** - ‚úÖ SCOPED (paper.tex:60, 398):
  - OLD: "raw model capability alone cannot bridge the gap"
  - NEW: "for CRUD-oriented web applications...complements model capability in achieving production reliability"
  - Added domain qualifiers: "within constrained domains", "in structured domains"

#### Acknowledge Limitations
- [ ] **Address CRUD-only scope** explicitly in limitations
- [ ] **Discuss extension to complex/ambiguous domains**
- [ ] **Note human evaluation subjectivity** and propose alternatives

### 10. Community Adoption Validation ‚úÖ **ADDRESSED**

- [x] **Add production evidence** - ‚úÖ ADDED (new subsection: Production Deployment and Community Adoption)
  - 650 GitHub stars, 89 forks (October 2025) - ‚úÖ Quantified community traction
  - Hundreds of apps generated daily at peak usage - ‚úÖ Production scale documented
  - Thousands of accumulated deployments - ‚úÖ Long-term viability demonstrated
  - Positioned as "ecological validity" complementing controlled experiments
  - Updated abstract and contributions to emphasize production deployment
  - **Note:** No visualization added (would require figure space), but metrics integrated throughout
- [ ] **Resolve counting inconsistency:**
  - Table 3: 22/30 viable (73.3%)
  - Section 5.1: "among viable applications (V=1, n=21)"
  - Reconcile numbers

### 11. Minor Structural Issues

- [ ] **Section 4.1 "Evaluation Framework"** - Currently empty/hanging:
  - Fill with content OR remove section header
- [ ] **Consolidate Sections 5.3 and 5.4** - Consider merging for better flow

---

## üìä Priority Summary

### Blocking Issues (Must Fix for Future Submission)
1. ‚úÖ Fix uncited quantitative claims in Section 2.2 (CRITICAL)
2. Add OpenHands baseline comparison
3. Expand dataset to ‚â•100 prompts
4. Release reproducibility artifacts (prompts, Dockerfiles, rubrics)

### Strongly Recommended (Major Impact)
5. Evaluate on established benchmarks (SWE-bench, HumanEval)
6. Replicate evaluation across all three stacks
7. Formalize AB rubric with precise criteria
8. Add qualitative trajectory analysis
9. Moderate overstated claims

### Nice to Have (Polish)
10. Community adoption visualization
11. E2E brittleness root cause investigation
12. Token-based cost reporting
13. Structural cleanup (empty sections, consolidation)

---

## Next Steps for SANER Industrial Track

### Already Addressed in Current SANER Revision ‚úÖ

**üî¥ CRITICAL Issues (All 3 Addressed):**
- [x] **Section 2.2 uncited claims** - ALL FIXED (removed uncited, added citations for AST-T5/TiCoder)
- [x] **OpenHands baseline** - Cited and positioned in Related Work (not experimentally evaluated)
- [x] **Tech stack justification** - Added explanation in Architecture section

**Related Work Improvements:**
- [x] Condensed to ~0.5 page (from 4 subsections ‚Üí 2 paragraphs)
- [x] Added modern 2024-2025 baselines (OpenHands ICLR 2025, SWE-agent, AutoCodeRover, Agentless)
- [x] Added Kniazev 2008 early work citation (modest framing)
- [x] Added 4 new citations: wang2024openhands, kniazev2008automated, pan2024ticoder, gong2024astt5
- [x] Removed all subsection headers, created unified "Background and Related Work"
- [x] Modernized framing (2024 agents positioned as contemporary, not "older")

**Impact:** All 3 CRITICAL blocking issues from SEA reviews are now addressed at the paper level. Remaining items require either: (1) new experiments (dataset expansion, benchmarks), (2) artifact release, or (3) methodology documentation.

### Still Needed for Strong SANER Submission
1. Keep focus on industrial context (production reliability, cost, maintainability)
2. Emphasize practical lessons over benchmark numbers
3. Frame limitations honestly (CRUD-focused, 30 prompts)
4. Consider adding "Lessons for Practitioners" section
5. Release artifacts alongside submission if permitted

### Consider for Future Workshop/Journal Version
- Expand to 100+ prompts
- Multi-stack evaluation
- SWE-bench comparison
- Full reproducibility package
