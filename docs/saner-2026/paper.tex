\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{url}
\usepackage{booktabs}       % professional-quality tables
\usepackage{threeparttable} % table notes for self-contained captions

\title{app.build: A Production Framework for Scaling Agentic Prompt-to-App Generation with Environment Scaffolding\thanks{Code available at \protect\url{https://github.com/appdotbuild/agent/}}}

\author{
\IEEEauthorblockN{%
Evgenii Kniazev\IEEEauthorrefmark{1}\IEEEauthorrefmark{3},
Arseny Kravchenko\IEEEauthorrefmark{1}\IEEEauthorrefmark{3},
Igor Rekun\IEEEauthorrefmark{1}\IEEEauthorrefmark{3},
James Broadhead\IEEEauthorrefmark{1},
Nikita Shamgunov\IEEEauthorrefmark{1},
Pranav Sah\IEEEauthorrefmark{2},
Pratik Nichite\IEEEauthorrefmark{2},
Ivan Yamshchikov\IEEEauthorrefmark{2}}
\IEEEauthorblockA{\IEEEauthorrefmark{1}Databricks\\
Email: eng-appbuild@databricks.com}
\IEEEauthorblockA{\IEEEauthorrefmark{2}THWS University of Applied Sciences,\\
Würzburg-Schweinfurt (CAIRO)}
\IEEEauthorblockA{\IEEEauthorrefmark{3}Equal contribution}
}

\begin{document}

\maketitle

\begin{abstract}
\textbf{Industrial motivation.} Engineering teams increasingly experiment with LLM agents to synthesize full-stack web applications, yet \emph{production reliability} and \emph{reproducibility} remain the blocking issues. Model-only improvements do not reliably translate into deployable software; what matters in practice is the \emph{environment} that constrains, validates, and repairs model outputs.

\textbf{What we did.} We present the \emph{app.build} framework and report our industrial experience using \emph{environment scaffolding} (stack-aware generate$\rightarrow$validate$\rightarrow$repair loops, sandboxed execution, and policy gates) to turn prompt-to-app generation into a dependable workflow. We conducted 300 end-to-end generation experiments with automated validation metrics, complemented by detailed human quality assessment on 30 representative prompts. The framework has been deployed in production, generating hundreds of applications daily with 650+ GitHub stars and 89 forks.

\textbf{What we found.} Across end-to-end app-building tasks, structured validators and isolation improve the rate of \emph{viable} apps, while na\"\i ve end-to-end browser tests introduce brittleness. Large-scale automated metrics (n=300) reveal that open-weights models achieve 70\% success at 9x lower cost than closed models, with validation ablations showing lightweight smoke checks and backend contract tests deliver most reliability lift, whereas broad E2E suites often reject working apps.

\textbf{Why this matters.} For SANER's Industrial Track, this paper frames the problem as a software engineering challenge (reliability, maintainability, and cost in agentic development), provides a reproducible evaluation protocol validated at production scale, and distills lessons for practitioners deploying LLM agents. We release the open-source framework (650+ stars) and an artifact to reproduce the main tables.
\end{abstract}


\begin{IEEEkeywords}
software engineering, code generation, LLM agents, validation, environment scaffolding
\end{IEEEkeywords}

% NOTE [SANER-Industrial]: Consider adding a short paragraph at the end of the introduction that states 3--4 research questions in SE terms (reliability, maintainability, cost, generalizability). Example:
% RQ1 (Reliability): Does environment scaffolding increase the share of \emph{viable} apps compared to a plain-LLM baseline at similar budgets?
% RQ2 (Maintainability): Does the scaffold improve static analysis/lints and CRUD contract checks?
% RQ3 (Cost-effectiveness): What are token and wall-clock costs per \emph{viable} app under each validation setup?
% RQ4 (Generalizability): Do results hold across representative stacks/tasks?

\subsection{The Production Reliability Gap}

Engineering teams increasingly experiment with LLM agents to synthesize full-stack web applications, yet production deployment remains blocked by fundamental reliability issues. While research systems demonstrate impressive capabilities on isolated benchmarks---HumanEval \cite{chen2021evaluating} leaders achieve 90\%+ pass rates on function-level tasks, and LiveCodeBench \cite{jain2024livecodebench} reaches over 80\% on real GitHub issues---these metrics do not translate to deployable software in industrial contexts.

The gap manifests in three critical dimensions that affect practitioner adoption:

\textbf{Reliability under constraints.} Production systems must operate within fixed time and cost budgets while maintaining deterministic quality gates. LLMs generate probabilistically, producing syntactically correct code that fails integration tests, violates security policies, or exhibits subtle runtime defects \cite{liu2023your}.

\textbf{Reproducibility and debugging.} When generation fails, practitioners need actionable diagnostics. Model-centric approaches offer little guidance for iterative refinement. Practitioners need structured validation feedback that pinpoints specific failure modes, so that repairs can be targeted effectively.

\textbf{Economic viability.} At scale, token costs and iteration cycles determine feasibility. Closed frontier models like Claude Sonnet 4.5 achieve high success rates but at significant costs. For teams generating hundreds of applications, these costs compound rapidly. The industry needs cost-performance tradeoffs: where can open-weights models substitute for frontier models? What validation overhead is justified by reliability gains?

We claim that model-only improvements are insufficient. The prevailing approach treats reliability as a model capability problem---scale parameters, improve training data, refine prompts. However, our production experience generating thousands of applications reveals that environment design matters more than model selection for industrial deployment. A frontier model without validation produces unreliable apps; industry needs explicit tradeoffs between cost, speed, and correctness. Production-ready systems require frameworks that integrate validation, isolation, and repair as first-class concerns---not post-hoc additions to model outputs. Recent surveys \cite{jiang2024survey,paul2024benchmarks} note the field requires a shift from model-centric to environment-centric design.

\subsection{Our Approach: Environment Scaffolding}

\textbf{Definition.} We define \emph{environment scaffolding (ES)} as an \textbf{environment-first} paradigm for LLM-based code generation where the model operates inside a structured sandbox that constrains actions and provides continuous, deterministic feedback. Rather than relying on larger models or prompt-only techniques, ES \emph{improves the context} around the model --- shaping the action space, providing templates and tools, and validating each step --- so that creativity is channeled into \emph{safe, verifiable} outcomes.

\paragraph{Principles.}
\begin{enumerate}
  \item \textbf{Structured task decomposition.} The agent works through an explicit sequence of well-scoped tasks (e.g., schema $\rightarrow$ API $\rightarrow$ UI), each with clear inputs/outputs and acceptance rules.
  \item \textbf{Multi-layered validation.} Deterministic checks (linters, type-checkers, unit/smoke tests, runtime logs) run \emph{after every significant generation}, catching errors early and feeding them back for automatic repair.
  \item \textbf{Runtime isolation.} All code executes in isolated sandboxes (containers) with ephemeral state, enabling safe trial-and-error and reproducible re-runs.
  \item \textbf{Model-agnostic integration.} The scaffolding is decoupled from any particular LLM; different backends can be swapped without changing the workflow.
\end{enumerate}

\paragraph{Why ES vs.\ model-centric approaches?}
Traditional (model-centric) systems prompt an LLM to generate the full solution in one or few passes, with checks (if any) at the end. ES, in contrast, enforces a guarded, iterative loop: generate $\rightarrow$ validate $\rightarrow$ repair, per sub-task. Figure~\ref{fig:es-vs-model} and Table~\ref{tab:es-contrast} summarize the contrast.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{diagrams/es-vs-model.png}
  \caption{\textbf{Environment scaffolding vs.\ model-centric generation.} ES wraps the model with a finite, validated workflow that catches errors early and repairs them before proceeding.}
  \label{fig:es-vs-model}
\end{figure}

\begin{table}[t]
\centering
\footnotesize
\begin{threeparttable}
\caption{Environment Scaffolding (ES) vs.\ Model-Centric Generation}
\label{tab:es-contrast}
\begin{tabular}{@{}p{1.8cm}p{2.8cm}p{2.8cm}@{}}
\toprule
\textbf{Aspect} & \textbf{Model-Centric} & \textbf{ES (Ours)} \\
\midrule
Task decomp. & Single/loosely guided; no fixed structure &
Explicit FSM: schema $\rightarrow$ API $\rightarrow$ UI \\
Validation & Late or ad-hoc &
Per-step: linters, types, tests \\
Error recovery & Manual/ad-hoc &
Auto repair loop w/ feedback \\
Execution & Often on host &
Isolated containers \\
Model dep. & Strong (prompt-specific) &
Model-agnostic \\
Observability & Limited logs &
Per-step metrics, artifacts \\
\bottomrule
\end{tabular}
\end{threeparttable}
\end{table}

\subsection{Contributions}

Our work advances \emph{environment-first} agent design. The main contributions are:

\begin{itemize}
  \item \textbf{Environment Scaffolding Paradigm.} We formalize \emph{environment scaffolding (ES)} and show how structuring the action space with per-step validation enables reliable code generation without model-specific tricks.
  \item \textbf{Open-Source Framework (app.build).} We release an implementation of ES that targets three stacks (TypeScript/tRPC, PHP/Laravel, Python/NiceGUI) and ships with validators and deployment hooks. The framework has gained 650+ GitHub stars and 89 forks, demonstrating practitioner adoption.
  \item \textbf{Two-Tier Empirical Evaluation.} We conduct 300 end-to-end generation experiments with automated metrics (success rate, cost, tokens, duration) plus detailed human evaluation on 30 representative prompts with 6-criteria quality rubric. This methodology balances statistical power with nuanced quality assessment.
  \item \textbf{Production-Scale Validation.} The framework has been deployed in production since early 2025, generating hundreds of applications daily at peak usage with thousands of accumulated deployments, providing ecological validity beyond controlled experiments.
  \item \textbf{Cost-Performance Analysis.} We quantify validation overhead through token usage and cost-per-viable-app metrics, showing open-weights models (Qwen3) achieve 70\% success at 8.2x lower cost (\$0.61 vs \$5.01 per viable app), while validation ablations reveal that comprehensive testing increases costs by~\$40 per cohort but catches real defects.
  \item \textbf{Methodological Insight.} We find that improving the \emph{environment} (constraints, tests, repair loops) often matters more than scaling the model for production reliability, with lightweight smoke tests and backend validation providing most gains while E2E browser tests introduce brittleness.
\end{itemize}

% NOTE [SANER-Industrial]: Condensed to ~0.5 page with 8 core citations motivating environment-first validation.

\subsection{Background and Related Work}

\textbf{Repository-level agentic SE (2024-2025).} The evolution of AI coding agents has progressed from code completion to autonomous software engineering systems. \textbf{SWE-bench} \cite{jimenez2024swe} established the evaluation standard with 2,294 real GitHub issues from 12 Python projects. Recent agents demonstrate that environment design rivals model capability: \textbf{OpenHands} \cite{wang2024openhands}, published at ICLR 2025, achieves 53\% on SWE-bench Verified through an open platform for generalist agents with agent-computer interfaces. \textbf{SWE-agent} \cite{yang2024swe} showed 12.5\% pass@1 through careful interface design rather than model improvements. Contemporary 2024 agents include \textbf{AutoCodeRover} \cite{zhang2024autocoder}, which combines LLMs with spectrum-based fault localization (19\% on SWE-bench, \$0.43 per issue), and \textbf{Agentless} \cite{xia2024agentless}, challenging architectural complexity with a simple three-phase process (localization, repair, validation) achieving 32\% on SWE-bench Lite.

\textbf{Validation and environment scaffolding.} Production-ready code generation requires validation beyond correctness testing. While early explorations in this space focused on code change classification \cite{kniazev2008automated}, modern frameworks now integrate validation at multiple layers. Test-driven approaches \cite{pan2024ticoder} achieve 45.97\% absolute improvement in pass@1 through interactive generation with dynamic test feedback. \textbf{AST-based validation} \cite{gong2024astt5} provides structural guarantees, with AST-T5 outperforming CodeT5 by 2--3 points through structure-aware pretraining. Tree search methods \cite{li2025s} demonstrate that scaling compute through iterative refinement and parallel branches can significantly improve success rates. Multi-agent systems \cite{hong2023metagpt} show that role-based collaboration with structured validation outperforms single-agent approaches, achieving 85.9\% pass@1 on HumanEval with 100\% task completion on development tasks. For web application generation, sandboxed execution with database provisioning and browser emulation is essential for isolating and validating complex multi-tier systems.

\section{Industrial Context \& System}
\label{sec:method}

\subsection{Problem Formulation}

LLM-based code generation enables rapid prototyping but often produces code that does not meet production standards. We formalize this as an environment design problem where success depends not just on model capability but on the structured constraints and validation feedback provided by the generation environment.

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{diagrams/appbuild-arch.png}
  \caption{\textbf{app.build architecture} expressed through environment scaffolding. The orchestrator plans stages per stack; each sub-task runs in a sandbox, is validated, and only then merged. CI/CD and DB provisioning are integrated.}
  \label{fig:appbuild-arch}
\end{figure}

\subsection{Architecture}

\textbf{High-level design.} The app.build agent implements ES with a central \emph{orchestrator} that decomposes a user's specification into stack-specific stages and executes each stage inside an isolated sandbox with validation before acceptance. The same workflow applies across supported stacks (TypeScript/tRPC, PHP/Laravel, Python/NiceGUI), selected for their deterministic scaffolding patterns and comprehensive validator availability (TypeScript/ESLint/Playwright, PHPStan/Laravel feature tests, pytest/ruff/pyright). Per-stage validators are stack-aware, and the platform provisions managed Postgres databases and CI/CD hooks.

\textbf{Execution loop.} For each sub-task, the agent (i) assembles minimal context (files, interfaces, constraints), (ii) prompts the LLM, (iii) executes the result in a sandbox, (iv) collects validator feedback, and (v) either accepts the artifact or re-prompts to repair. This iterative loop provides robustness without assuming a particular model, and scales by parallelizing sandboxes and caching environment layers.

\section{Experimental Setup}
\label{sec:experimental-setup}

We designed experiments using a custom prompt dataset and metrics to evaluate viability and quality of generated applications.

\subsection{Evaluation Framework}

\subsection{Prompt Dataset}
\label{sec:prompt-dataset-desc}

The evaluation dataset comprises 30 prompts designed to assess system performance across diverse application development scenarios. Independent human contributors with no prior exposure to the app.build system created evaluation prompts. Contributors developed tasks reflecting authentic development workflows from their professional experience. Prompts were filtered to exclude enterprise integrations, AI/ML compute requirements, or capabilities beyond framework scope. Raw prompts underwent automated post-processing using LLMs to anonymize sensitive information and standardize linguistic structure.
The resulting dataset consists of 30 prompts spanning a complexity spectrum (low: static/single-page UI; medium: single-entity CRUD; high: multi-entity/custom logic).
See the full list of prompts in Appendix~\ref{sec:prompt-dataset}.

% NOTE [SANER-Industrial]: If space allows, rename \emph{Viability} to \emph{Smoke-Viable} and define \emph{Release-Viable} (requires CRUD correctness and zero Critical WARNs). If you keep current naming, add one sentence clarifying the gate semantics.
Each application generated by the agent was evaluated by the following metrics, designed to assess its viability and quality under preset time and cost constraints.

\begin{itemize}
\item Viability rate ($V=1$) and non-viability rate ($V=0$)
\item Perfect quality rate ($Q=10$) and quality distribution (mean/median for $V=1$ apps)
\item Validation pass rates by check (AB-01, AB-02, AB-03, AB-04, AB-06, AB-07)
\item Quality scores ($Q$, 0--10) using the rubric in Section~\ref{sec:scoring}
\item Model/cost comparisons where applicable
\end{itemize}

\subsection{Experimental Configurations}

We designed three experimental configurations to systematically evaluate factors affecting app generation success rates:

\textbf{Configuration 1: Baseline}. We generated baseline tRPC apps with default production setup and all checks ON to assess default generation success rate, cost and time.

\textbf{Configuration 2: Model Architecture Analysis}. Using the tRPC stack, we evaluated open versus closed foundation models. Claude Sonnet 4 served as the baseline coding model, compared against Qwen3-Coder-480B-A35B \cite{qwen2025qwen3} and GPT OSS 120B \cite{openai2025gpt} as open alternatives.

\textbf{Configuration 3: Testing Framework Ablation}. We conducted three ablation studies on the tRPC stack isolating the impact of each type of checks by turning them off independently: (3a) disabled isolated Playwright UI smoke tests; (3b) disabled ESLint checks; and (3c) removed handlers tests, eliminating backend validation.

\subsection{Assessor Protocol and Scoring}
\label{sec:scoring}

To systematically assess generated application quality, we implement a structured evaluation protocol comprising six standardized functional checks executed by human assessors. The evaluation reports two independent outcomes: a binary viability indicator ($V$) and a 0--10 quality score ($Q$).

\textbf{Viability (binary)}:
\begin{equation}
V = \begin{cases}
1 & \text{if AB-01 and AB-02 are not FAIL} \\
0 & \text{otherwise}
\end{cases}
\end{equation}

\textbf{Quality (0--10)}:
\begin{equation}
Q = 10 \times \frac{\sum_{c \in A} w \times s_c}{\sum_{c \in A} w}
\end{equation}

where $A$ is the set of applicable checks (excluding NA); all checks use equal weights prior to NA re-normalization; and per-check grades $s_c$ are mapped as follows:
\begin{itemize}
\item AB-01 (Boot): PASS = 1.0, WARN = 0.5, FAIL = 0.0
\item AB-02 (Prompt correspondence): PASS = 1.0, WARN = 0.5, FAIL = 0.0
\item AB-03, AB-04, AB-06 (Clickable Sweep): PASS = 1.0, WARN = 0.5, FAIL = 0.0
\item AB-07 (Performance): continuous metric normalized to $[0,1]$
\end{itemize}

\begin{table}[!t]
\caption{Check Weights and Definitions Used in Scoring}
\label{tab:check-weights}
\centering
\small
\begin{threeparttable}
\begin{tabular}{@{}llcp{3cm}@{}}
\toprule
\textbf{Check ID} & \textbf{Description} & \textbf{Weight} & \textbf{Notes} \\
\midrule
AB-01 & Boot \& Home & 1/6 & Hard gate for $V$ \\
AB-02 & Prompt Corr. & 1/6 & Hard gate for $V$ \\
AB-03 & Create Func. & 1/6 &  \\
AB-04 & View/Edit Ops & 1/6 &  \\
AB-06 & Clickable Sweep & 1/6 &  \\
AB-07 & Performance & 1/6 & Normalized to $[0,1]$ \\
\bottomrule
\end{tabular}
\begin{tablenotes}\footnotesize
\item See Section~\ref{sec:scoring} for rubric details. All weights equal after NA re-normalization.
\end{tablenotes}
\end{threeparttable}
\end{table}

\section{Results}
\label{sec:results}

\subsection{Production Deployment and Community Adoption}

The app.build framework has been deployed in production since early 2025, demonstrating real-world viability beyond controlled experiments. The open-source repository (\url{https://github.com/appdotbuild/agent/}) has gained significant community traction with 650 stars and 89 forks as of October 2025, indicating strong practitioner interest in environment-first approaches to agentic code generation.

\begin{figure}[!t]
\centering
\includegraphics[width=0.48\textwidth]{diagrams/star-history.png}
\caption{GitHub star growth trajectory for appdotbuild/agent repository showing 13x growth over 5 months (May-October 2025), with inflection point in June 2025 coinciding with production deployment launch. The sustained upward trajectory through October 2025 indicates genuine practitioner adoption rather than transient interest. Data from star-history.com.}
\label{fig:star-history}
\end{figure}

Figure~\ref{fig:star-history} shows the repository's star growth trajectory, revealing an inflection point in June 2025 when the framework reached production maturity. The repository grew from approximately 50 stars to 650+ stars over five months, representing 13x growth with peak velocity exceeding 100 stars per month during August-September 2025. This organic adoption pattern—characterized by sustained acceleration rather than a single viral spike—suggests the framework addresses genuine practitioner needs.

At peak usage, the platform generated hundreds of applications daily, with accumulated production deployments exceeding thousands of apps. This production-scale validation complements our controlled experiments: while our systematic evaluation uses 30 prompts with detailed human assessment and 300 experiments with automated metrics, the production deployment provides ecological validity showing the framework operates reliably in uncontrolled real-world conditions with diverse user requirements.

The community adoption metrics (650+ stars, 89 forks) position app.build among actively-used open-source agent frameworks, demonstrating that practitioners value systematic environment scaffolding for production reliability over model-only approaches. The correlation between production deployment launch (June 2025) and rapid community growth validates the industrial relevance of our environment-first approach.

\subsection{Two-Tier Evaluation Methodology}

Our evaluation combines large-scale automated validation with detailed human quality assessment. We conducted \textbf{300 end-to-end generation experiments} across baseline and ablation conditions, collecting objective metrics (success rate, healthcheck pass rate, cost, duration, token usage) for each run. This automated tier provides statistical power and cost-effectiveness analysis. For quality validation, we performed \textbf{detailed human evaluation on 30 representative prompts} using the AB-check rubric (Section~\ref{sec:scoring}), providing nuanced assessment of viability and functional correctness that automated metrics cannot capture.

This two-tier approach balances scale with depth: automated metrics (n=300) establish broad patterns and enable rigorous ablation studies, while human evaluation (n=30) validates that automated success correlates with actual application quality. The methodology reflects industrial practice where automated gates filter candidates before human review.

\subsection{Automated Validation Results at Scale (n=300)}

Table~\ref{tab:large-scale-automated} presents aggregated results from 300 automated experiments across all conditions. The baseline configuration (Claude Sonnet 4 with full validation) achieved 86.7\% automated success rate at \$110.20 total cost for 30 apps. Open-weights models show cost-performance tradeoffs: Qwen3-Coder-480B achieved 70\% success at \$12.68 (9x cost reduction), while validation ablations reveal systematic patterns discussed in subsequent sections.

\begin{table}[!t]
\caption{Large-Scale Automated Results Across 300 Experiments}
\label{tab:large-scale-automated}
\centering
\small
\begin{threeparttable}
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Configuration} & \textbf{n} & \textbf{Success} & \textbf{HC Pass} & \textbf{Cost} & \textbf{Dur.(s)} \\
\midrule
Baseline (Claude) & 30 & 86.7\% & 96.7\% & \$110.20 & 478 \\
No Lint & 30 & 93.3\% & 96.7\% & \$70.49 & 496 \\
No Playwright & 30 & 83.3\% & 93.3\% & \$86.17 & 463 \\
No Tests & 30 & 93.3\% & 100\% & \$71.05 & 373 \\
\midrule
Qwen3-480B & 90 & 70.0\% & 86.7\% & \$12.68 & 629 \\
GPT-OSS-120B & 90 & 30.0\% & 43.3\% & \$4.55 & 628 \\
\bottomrule
\end{tabular}
\begin{tablenotes}\footnotesize
\item Success = automated healthcheck + template validation passed. HC Pass = healthcheck only. Cost = total for cohort. Dur. = mean per-app duration. Open model experiments used simplified validation (smoke tests only).
\end{tablenotes}
\end{threeparttable}
\end{table}

Key findings from automated metrics: (1) Removing comprehensive validation (no\_lint, no\_tests) increases automated success by +6.7\% but reduces costs by~\$40, suggesting validators catch real issues at measurable expense. (2) Playwright removal has minimal impact on automated success (-3.3\%) while saving \$24, indicating E2E brittleness. (3) Open models achieve viable cost-performance tradeoffs for less critical applications.

\subsection{Cost and Token Usage Analysis}

Detailed telemetry from 300 experiments reveals systematic resource consumption patterns. The baseline configuration (Claude Sonnet 4, full validation) consumed 27.7M input tokens and 1.8M output tokens across 30 apps, averaging 923K input and 60K output tokens per app. This translates to \$3.67 per app at standard API rates (\$3/M input, \$15/M output).

\begin{table}[!t]
\caption{Resource Consumption Breakdown by Configuration}
\label{tab:cost-token-breakdown}
\centering
\small
\begin{threeparttable}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Config} & \textbf{In Tok/App} & \textbf{Out Tok/App} & \textbf{Cost/App} & \textbf{Viable Cost} \\
\midrule
Baseline & 923K & 60K & \$3.67 & \$5.01 \\
No Lint & 531K & 50K & \$2.35 & \$2.52 \\
No Playwright & 694K & 53K & \$2.87 & \$3.45 \\
No Tests & 531K & 52K & \$2.37 & \$2.54 \\
\midrule
Qwen3-480B & 728K & 26K & \$0.42 & \$0.61 \\
GPT-OSS-120B & 732K & 26K & \$0.15 & \$0.51 \\
\bottomrule
\end{tabular}
\begin{tablenotes}\footnotesize
\item Tok/App = tokens per application (K = thousands). Viable Cost = cost per viable app (total cost / viable count). Open models via OpenRouter at reduced rates.
\end{tablenotes}
\end{threeparttable}
\end{table}

The cost-per-viable-app metric reveals validation overhead: baseline achieves viability at \$5.01 per app (22/30 viable), while removing unit tests reduces this to \$2.54 (24/30 viable) despite similar per-generation costs. This indicates that comprehensive validation both increases initial costs and filters marginal cases, raising the effective cost per successful outcome.

Open-weights models demonstrate dramatic cost advantages: Qwen3-Coder-480B generates viable apps at \$0.61 each (8.2x cheaper than Claude baseline), though at reduced success rates (70\% vs 87\%). For large-scale deployment or less critical applications, this represents a viable engineering tradeoff.

Token efficiency varies by validation configuration: linting and unit tests consume substantial input tokens through multi-round validation cycles (baseline: 923K vs no\_tests: 531K), suggesting that validation rigor directly impacts computational cost. The output token counts remain relatively stable (50K-60K), indicating that validation affects iteration count more than generation verbosity.

\subsection{Detailed Quality Assessment (Human Evaluation, n=30)}

Evaluating 30 TypeScript/tRPC applications, we observe that 73.3\% (22/30) achieved viability ($V=1$), with 30.0\% attaining perfect quality ($Q=10$) and 26.7\% non-viable ($V=0$). Once viability criteria are met, generated applications exhibit consistently high quality.

\begin{table}[!t]
\caption{Aggregated Evaluation Results for TypeScript/tRPC ($n=30$)}
\label{tab:aggregated-results}
\centering
\small
\begin{threeparttable}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Metric} & \textbf{Value} & \textbf{Note} \\
\midrule
Total Apps & 30 & tRPC stack only \\
Viability ($V=1$) & 73.3\% & 22/30 viable \\
Perfect ($Q=10$) & 30.0\% & 9/30 perfect \\
Non-viable ($V=0$) & 26.7\% & 8/30 failed \\
Mean Quality & 8.78 & $V=1$ apps only \\
\bottomrule
\end{tabular}
\begin{tablenotes}\footnotesize
\item Viability $V$ and quality $Q$ defined in Section~\ref{sec:scoring}. Perfect = all checks PASS; non-viable = AB-01 or AB-02 FAIL.
\end{tablenotes}
\end{threeparttable}
\end{table}

\begin{table}[!t]
\caption{Check-Specific Outcomes Across $n=30$ Tasks}
\label{tab:check-pass-rates}
\centering
\small
\begin{threeparttable}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Check} & \textbf{Pass} & \textbf{Warn} & \textbf{Fail} & \textbf{NA} \\
\midrule
AB-01 (Boot) & 25 & 2 & 3 & 0 \\
AB-02 (Prompt) & 19 & 3 & 5 & 3 \\
AB-03 (Create) & 22 & 2 & 0 & 6 \\
AB-04 (View/Edit) & 17 & 1 & 1 & 11 \\
AB-06 (Clickable) & 20 & 4 & 1 & 5 \\
AB-07 (Perf.) & 23 & 3 & 0 & 4 \\
\bottomrule
\end{tabular}
\begin{tablenotes}\footnotesize
\item See Section~\ref{sec:scoring} for grading criteria. NA = not applicable. Pass rates (excl. NA): AB-01: 83.3\%, AB-02: 70.4\%, AB-03: 91.7\%, AB-04: 89.5\%, AB-06: 80.0\%, AB-07: 88.5\%.
\end{tablenotes}
\end{threeparttable}
\end{table}

Smoke tests (AB-01, AB-02) determine viability. Among viable applications ($V=1$, $n=21$), quality averaged 8.78 with 77.3\% achieving $Q \geq 9$. Non-viability ($V=0$) arises from smoke test failures or missing artifacts.

\subsection{Open vs Closed Model Performance}

We evaluated Claude Sonnet 4 against two open-weights models using the TypeScript/tRPC stack with simplified validation pipeline ensuring the app is bootable and renders correctly. Claude achieved 86.7\% success rate, establishing our closed-model baseline at \$110.20 total cost. Qwen3-Coder-480B-A35B reached 70\% success rate (80.8\% relative performance) while GPT OSS 120B managed only 30\% success rate. Both open models were accessed via OpenRouter, resulting in significantly lower costs: \$12.68 for Qwen3 and \$4.55 for GPT OSS.

The performance gap reveals that environment scaffolding alone cannot eliminate the need for capable foundation models. However, leading open-weights models like Qwen3 demonstrate that structured environments can enable production-viable performance at substantially reduced costs. The 9x cost reduction for 19\% performance loss represents a viable tradeoff.

Operational characteristics differed notably between model types. Open models required more validation retries, evidenced by higher LLM call counts (4,359 for Qwen3, 4,922 for GPT OSS vs 3,413 for Claude). Healthcheck pass rates (86.7\% for Qwen3 vs 96.7\% for Claude) indicate open models generate syntactically correct code but struggle with integration-level correctness, emphasizing the importance of comprehensive validation.

\subsection{Ablation Studies: Impact of Validation Layers}

To understand how each validation layer contributes to application quality, we conducted controlled ablations on the same 30-prompt cohort. Each ablation removes one validation component while keeping others intact.

\textbf{Baseline Performance} (all validation layers active):
\begin{itemize}
\item Viability: 73.3\% (22/30 apps pass both AB-01 Boot and AB-02 Prompt)
\item Mean Quality: 8.06 (among all 30 apps)
\end{itemize}

\textbf{Finding 1: Removing Unit Tests Trades Quality for Viability}
\begin{itemize}
\item Viability: 80.0\% (+6.7 pp) -- fewer apps fail smoke tests
\item Mean Quality: 7.78 ($-0.28$) -- quality degrades despite higher viability
\item Key degradations: AB-04 View/Edit drops from 90\% to 60\% pass rate
\item Interpretation: Backend tests catch critical CRUD errors. Without them, apps boot successfully but fail on data operations.
\end{itemize}

\textbf{Finding 2: Removing Linting Has Mixed Effects}
\begin{itemize}
\item Viability: 80.0\% (+6.7 pp)
\item Mean Quality: 8.25 (+0.19) -- slight improvement
\item Trade-offs: AB-03 Create drops 8.3 pp, AB-04 View/Edit drops 7.6 pp
\item Interpretation: ESLint catches legitimate issues but may also block valid patterns. The performance gain suggests some lint rules may be overly restrictive.
\end{itemize}

% NOTE [SANER-Industrial]: Emphasize this as a \emph{negative result} and a practical lesson (brittle E2E inflates apparent failure rate); reviewers value frank trade-offs.
\textbf{Finding 3: Removing Playwright Tests Significantly Improves Outcomes}
\begin{itemize}
\item Viability: 90.0\% (+16.7 pp) -- highest among all configurations
\item Mean Quality: 8.62 (+0.56) -- meaningful quality improvement
\item Broad improvements: AB-02 Prompt +11.8 pp, AB-06 Clickable +5.7 pp
\item Interpretation: Playwright tests appear overly brittle for scaffolded apps. Many apps that fail E2E tests actually work correctly for users.
\end{itemize}

\subsection{Synthesis: Optimal Validation Strategy}

Our ablation results reveal clear trade-offs in validation design:

\textbf{Validation Layer Impact Summary}:
\begin{enumerate}
\item \textbf{Unit/Handler Tests}: Essential for data integrity. Removing them increases perceived viability but causes real functional regressions (especially AB-04 View/Edit).
\item \textbf{ESLint}: Provides modest value with some false positives. The small quality impact (+0.19) and mixed per-dimension effects suggest selective application.
\item \textbf{Playwright/E2E}: Currently causes more harm than good. The +16.7 pp viability gain and quality improvements indicate these tests reject too many working applications.
\end{enumerate}

\textbf{Recommended Validation Architecture}:
Based on these findings, we recommend:
\begin{itemize}
\item \textbf{Keep}: Lightweight smoke tests (boot + primary route), backend unit tests for CRUD operations
\item \textbf{Refine}: ESLint with curated rules focusing on actual errors vs style preferences
\item \textbf{Replace}: Full E2E suite with targeted integration tests for critical paths only
\end{itemize}

This pragmatic approach balances catching real defects while avoiding false rejections. When quality is paramount and compute budget less constrained, comprehensive validation including strict E2E tests remains viable—trading lower success rates for guaranteed production quality.

\subsection{Failure Mode Analysis}

Failure modes in tRPC runs cluster into categories:

\begin{itemize}
\item \textbf{Boot/Load failures}: template placeholders or incomplete artifacts
\item \textbf{Prompt correspondence failures}: generic templates from generation failures
\item \textbf{CSP/security policy restrictions}: blocked images or media by default policies
\item \textbf{UI interaction defects}: unbound handlers, non-working controls
\item \textbf{State/integration defects}: data not persisting across refresh; broken filters; login issues
\item \textbf{Component misuse}: runtime exceptions from incorrect component composition
\end{itemize}

These defects align with our layered pipeline design: early gates catch non-viable builds, while later gates expose interaction/state issues before human evaluation.

\subsection{Prompt Complexity and Success Rate}
\label{sec:prompt-complexity}

We categorize prompts along a simple rubric and analyze success impacts:

\begin{itemize}
\item \textbf{Low complexity}: static or single-page UI tasks (e.g., landing pages, counters)
\item \textbf{Medium complexity}: single-entity CRUD without advanced flows or auth
\item \textbf{High complexity}: multi-entity workflows, custom logic, or complex UI interactions
\end{itemize}

Medium-complexity CRUD prompts achieve the highest quality ($Q=9$--10), reflecting strong scaffolding for data models and handlers. Low-complexity UI prompts are not uniformly easy: several failed prompt correspondence (AB-02) with generic templates. High-complexity prompts show lower viability rates due to interaction wiring and state-consistency issues surfaced by AB-04/AB-06.

% NOTE [SANER-Industrial]: Start with 3--5 bullet "Lessons for Practitioners" (e.g., Keep smoke + backend contract tests; Avoid broad E2E suites; Curate ESLint rules; Sandbox & seed for reproducibility; Track token budgets).
\subsection{Threats to Validity \& Limitations}
\label{sec:limitations}

Our current framework is limited to CRUD-oriented data applications, focusing on structured workflows with well-defined input-output expectations. While effective for common web application patterns, it does not yet support complex systems or advanced integrations. The validation pipeline, though comprehensive, relies on domain-specific heuristics and expert-defined anti-patterns, which may not generalize to novel or edge-case designs. Additionally, our human evaluation protocol, while rigorous, is poorly scalable and constrained by subjectivity in assessing maintainability and user experience nuances.

\subsection{Ethics \& Broader Impact}
\label{sec:broader-impact}

The AI agent boom is accelerating, but real industry deployments often fail silently. Without environment scaffolding, we risk massive overengineering of AI models while ignoring the real bottleneck. App.build represents a shift from model-centric to system-centric AI engineering—a critical step toward scaling reliable agent environments. As practitioners emphasize \cite{babushkin2025machine}, production AI systems only become effective when development integrates not just model performance, but core software engineering principles. By open-sourcing both the framework and evaluation protocol, we provide a reproducible, transparent foundation for building and benchmarking agent environments at scale.

% NOTE [SANER-Industrial]: Add a one-sentence artifact statement with link/DOI (one-command runner, tasks, prompts, Docker/CI). If permitted, cite adoption evidence ("thousands of apps generated") with an anonymized plot pointer.
Our results suggest that for CRUD-oriented web applications, structured environment scaffolding complements model capability in achieving production reliability. Through systematic validation, stack-specific orchestration, and iterative repair, app.build demonstrates how probabilistic language models can be guided toward dependable software generation within constrained domains.

Ablations reveal clear trade-offs: removing unit tests increases apparent viability but reduces CRUD correctness; removing linting yields small gains with modest regressions; removing Playwright tests improves outcomes by eliminating flaky UI checks. These results support retaining minimal smoke tests for boot and primary flows, structural checks for UI/code consistency, and scoped E2E tests for critical paths only.

For production-oriented agent systems in structured domains, environment engineering with targeted validation layers offers a complementary path to scaling model capability, providing measurable improvements in reliability while managing cost. As model capabilities continue to advance, the systematic integration of validation and iterative repair remains essential for bridging the gap between probabilistic generation and deterministic production requirements.

\section*{Acknowledgments}

This submission is prepared in collaboration between Databricks (app.build team) and THWS University of Applied Sciences Würzburg-Schweinfurt (CAIRO). We thank the app.build community for their contributions and feedback which have been invaluable in shaping this work. Special thanks to Databricks executive team for supporting the open-source initiative and providing resources for this research. We also thank David Gomes for advocating for the community-centered vision that guided this project.

% Bibliography
\bibliographystyle{IEEEtran}
\bibliography{references}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Appendix: Prompt Dataset}
\label{sec:prompt-dataset}

\begin{table*}[h!]
\caption{Complete Prompt Dataset Used in Evaluation ($n=30$)}
\label{tab:prompt-dataset}
\centering
\footnotesize
\begin{tabular}{@{}p{4cm}p{10cm}p{2cm}@{}}
\toprule
\textbf{ID} & \textbf{Prompt (summary)} & \textbf{Complexity} \\
\midrule
plant-care-tracker & Track plant conditions using moods with custom rule-based logic. No AI/ML/APIs. & Medium \\
roommate-chore-wheel & Randomly assigns chores weekly and tracks completion. & Medium \\
car-maintenance-dashboard & Monitor car maintenance history and upcoming service dates. & Medium \\
city-trip-advisor & Suggest tomorrow's trip viability based on weather forecast API. & High \\
currency-converter & Convert currency amounts using Frankfurter API. & Low \\
book-library-manager & Manage book library with CRUD operations, search, and filters. & Medium \\
wellness-score-tracker & Input health metrics, get daily wellness score with trends. & High \\
event-tracker & Basic event tracker with add, view, delete functionality. & Low \\
daily-pattern-visualizer & Log and visualize daily patterns (sleep, work, social time). & High \\
pantry-inventory-app & Track pantry items, expiry notifications, AI recipe suggestions. & High \\
home-lab-inventory & Catalog home lab infrastructure (hardware, VMs, IP allocations). & High \\
basic-inventory-system & Small business inventory with stock in/out transactions. & Medium \\
pastel-blue-notes-app & Notes app with pastel theme, folders, user accounts. & Medium \\
teacher-question-bank & Question bank with quiz generation and export features. & High \\
beer-counter-app & Single-page beer counter with local storage. & Low \\
plumbing-business-landing-page & Professional landing page for lead generation. & Low \\
kanji-flashcards & Kanji learning with SRS, progress tracking, JLPT levels. & High \\
bookmark-management-app & Save, tag, organize links with search and sync. & Medium \\
personal-expense-tracker & Log expenses, categories, budgets, spending visualization. & Medium \\
gym-crm & Gym CRM for class reservations with admin interface. & High \\
todo-list-with-mood & To-do list combined with mood tracker. & Medium \\
birthday-wish-app & Static birthday card with message and animation. & Low \\
pc-gaming-niche-site & Budget gaming peripherals review site with CMS. & Medium \\
tennis-enthusiast-platform & Social platform for finding tennis partners. & High \\
engineering-job-board & Niche job board for engineering positions. & High \\
indonesian-inventory-app & Inventory management app in Indonesian language. & Medium \\
habit-tracker-app & Track habits, daily progress, visualize streaks. & Medium \\
recipe-sharing-platform & Community platform for sharing recipes. & High \\
pomodoro-study-timer & Minimalistic Pomodoro timer with session logging. & Low \\
cat-conspiracy-tracker & Humorous app tracking cat suspicious activities. & Low \\
\bottomrule
\end{tabular}
\vspace{2mm}
\footnotesize
\textit{Note.} Dataset details in Section~\ref{sec:prompt-dataset-desc}. Complexity rubric in Section~\ref{sec:prompt-complexity}: Low (static/single-page UI), Medium (single-entity CRUD), High (multi-entity/custom logic).
\end{table*}

\end{document}