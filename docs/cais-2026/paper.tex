%%
%% This is file for CAIS 2026 submission
%% ACM Conference on AI and Agentic Systems
%%

\documentclass[sigconf,review,anonymous]{acmart}

%% Rights management information
\setcopyright{acmlicensed}
\copyrightyear{2026}
\acmYear{2026}
\acmDOI{XXXXXXX.XXXXXXX}

%% Conference information
\acmConference[CAIS '26]{ACM Conference on AI and Agentic Systems}{May 26--29, 2026}{San Jose, CA, USA}
\acmBooktitle{Proceedings of the ACM Conference on AI and Agentic Systems (CAIS '26), May 26--29, 2026, San Jose, CA, USA}
\acmISBN{978-1-4503-XXXX-X/26/05}

%% Packages
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}

%% Code listing style
\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  backgroundcolor=\color{gray!10}
}

%%
%% Document starts
%%
\begin{document}

%%
%% Title
%%
\title{Klaudbiusz: Agent-Agnostic Tooling for Databricks Application Generation with Trajectory-Based Optimization and Composite Evaluation}

%%
%% Authors (anonymous for review)
%%
\author{Anonymous Authors}
\affiliation{%
  \institution{Anonymous Institution}
}
\email{anonymous@example.com}

%%
%% Abstract
%%
\begin{abstract}
We present Klaudbiusz, an open-source, agent-agnostic toolset for autonomous generation of Databricks data applications. Rather than building a specific agent, we provide reusable infrastructure---environment scaffolding, templates, and MCP tool integrations---that any agentic coding system can leverage. To continuously improve this toolset, we introduce a trajectory analyzer that processes agent execution traces via map-reduce LLM analysis, identifying friction points and generating actionable recommendations. To measure outcomes, we develop AppEval-100, a composite evaluation score built on four pillars: Reliability, SQL Quality (inspired by BIRD/Spider), Web Quality (inspired by WebArena), and Agentic DevX. Unlike existing benchmarks that evaluate SQL or web tasks in isolation, AppEval-100 provides the first composite evaluation for full-stack data applications, mapped to industry-standard DORA delivery metrics. We design a 100-prompt benchmark with $\pm$9\% confidence intervals across three difficulty tiers. Evaluated on 20 Databricks applications, we achieve 100\% build/runtime success with 6--9 minute generation latency at \$0.74 per application. We release the complete framework including scaffolding tools, trajectory analyzer, evaluation harness, and MLflow integration.
\end{abstract}

%%
%% CCS Concepts
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007.10011006.10011008.10011009.10011015</concept_id>
       <concept_desc>Software and its engineering~Software testing and debugging</concept_desc>
       <concept_significance>500</concept_significance>
   </concept>
   <concept>
       <concept_id>10010147.10010178.10010179</concept_id>
       <concept_desc>Computing methodologies~Natural language processing</concept_desc>
       <concept_significance>300</concept_significance>
   </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Software testing and debugging}
\ccsdesc[300]{Computing methodologies~Natural language processing}

%%
%% Keywords
%%
\keywords{agent-agnostic tooling, agentic code generation, evaluation framework, trajectory optimization, Databricks}

\maketitle

%%
%% Introduction
%%
\section{Introduction}
\label{sec:intro}

The emergence of agentic coding systems---AI agents capable of autonomously generating, testing, and deploying software applications---represents a paradigm shift in software engineering. While benchmarks like HumanEval~\cite{chen2021evaluating} and SWE-bench~\cite{jimenez2024swe} have advanced our understanding of code generation quality, they focus primarily on functional correctness rather than production readiness.

\textbf{The Infrastructure Gap.} We observe that agent performance depends heavily on the quality of surrounding infrastructure: environment scaffolding, templates, tool integrations, and validation pipelines. Yet most work focuses on improving agents themselves rather than the reusable tooling that enables them. We argue for an \emph{agent-agnostic} approach: build excellent infrastructure that any agent can leverage.

\textbf{Core Principle.} Our work is guided by a simple axiom: \emph{If an AI agent cannot autonomously deploy code using provided tooling, the tooling needs improvement---not necessarily the agent.}

\textbf{Three-Pillar Approach.} We address this through three interconnected contributions:

\begin{enumerate}
    \item \textbf{Agent-Agnostic Toolset.} We introduce Klaudbiusz, an open-source infrastructure for Databricks application generation comprising environment scaffolding, TypeScript/tRPC templates, MCP tool integrations, and containerized execution via Dagger. Any agentic system can leverage these tools.

    \item \textbf{Trajectory Analyzer.} To continuously improve this toolset, we present a map-reduce LLM approach for analyzing agent execution traces. The analyzer identifies friction points, inefficient patterns, and tool failures, generating actionable recommendations for infrastructure improvement.

    \item \textbf{Composite Evaluation (AppEval-100).} To measure outcomes, we develop a 4-pillar evaluation framework combining Reliability, SQL Quality, Web Quality, and Agentic DevX into a single composite score mapped to DORA delivery metrics.
\end{enumerate}

\textbf{Empirical Validation.} Evaluated on 20 Databricks applications, we achieve 100\% build/runtime success with 6--9 minute generation latency at \$0.74 per application.

%%
%% Related Work
%%
\section{Related Work}
\label{sec:related}

We survey evaluation approaches across four categories: function-level benchmarks, repository-level agent benchmarks, interactive agent environments, and evaluation harnesses. Table~\ref{tab:related-comparison} summarizes key frameworks and identifies the gap our work addresses.

\subsection{Function-Level Code Generation Benchmarks}

\textbf{HumanEval}~\cite{chen2021evaluating} introduced pass@k evaluation on 164 Python functions, becoming the standard metric for code generation. \textbf{MBPP}~\cite{austin2021program} expanded to 974 problems but remains algorithm-focused. Both benchmarks are now saturated---top models achieve $>$90\% on HumanEval---raising concerns about contamination and real-world relevance.

\textbf{BigCodeBench}\footnote{\url{https://github.com/bigcode-project/bigcodebench}} addresses these limitations with diverse library calls and complex instructions, while \textbf{LiveCodeBench}\footnote{\url{https://livecodebench.github.io/}} provides dynamic, contamination-resistant evaluation. Recent work on \textbf{HumanEval Pro and MBPP Pro}~\cite{humanevalpro2024} introduces self-invoking code generation to test progressive reasoning.

\textit{Gap:} Function-level benchmarks do not evaluate deployment, integration, or operational readiness.

\subsection{Repository-Level Agent Benchmarks}

\textbf{SWE-bench}~\cite{jimenez2024swe} evaluates agents on 2,294 real GitHub issues, with \textbf{SWE-bench Verified}\footnote{\url{https://openai.com/index/introducing-swe-bench-verified/}} providing 500 human-verified samples where top models achieve $\sim$72\%. The harder \textbf{SWE-bench Pro}\footnote{\url{https://scale.com/blog/swe-bench-pro}} reveals performance drops to $\sim$23\% on production-grade issues.

\textbf{SWE-agent}~\cite{yang2024swe} demonstrates that custom agent-computer interfaces significantly enhance performance through Docker-based harnesses and tool augmentation. \textbf{REPOCOD}\footnote{\url{https://arxiv.org/abs/2410.21647}} tests complete method implementation with only 6\% resolution rate, while \textbf{DevQualityEval}\footnote{\url{https://github.com/symflower/eval-dev-quality}} evaluates multi-language software engineering tasks on private datasets to avoid contamination.

\textit{Gap:} Repository-level benchmarks focus on patch correctness, not autonomous deployment capability.

\subsection{Text-to-SQL Benchmarks}

For data-centric applications, text-to-SQL benchmarks provide critical evaluation capabilities. \textbf{Spider}~\cite{yu2018spider} introduced cross-domain evaluation with 10,181 questions across 200 databases, establishing difficulty tiers (easy/medium/hard/extra-hard). \textbf{BIRD}~\cite{bird2023} scales to 12,751 question-SQL pairs across 95 databases (33.4GB), introducing the \emph{Valid Efficiency Score (VES)} that rewards both correctness and query efficiency.

\textbf{Spider 2.0}~\cite{spider2_2024} targets enterprise workflows with 632 real-world tasks involving BigQuery and Snowflake, where even o1-preview achieves only 21.3\% (vs 91.2\% on Spider 1.0). This dramatic performance gap highlights the challenge of production-grade SQL generation.

\textit{Gap:} Text-to-SQL benchmarks evaluate query correctness in isolation, not within deployed applications with UI, APIs, and DevOps requirements.

\subsection{Data Analytics Agent Benchmarks}

Recent benchmarks evaluate agents on end-to-end data analysis. \textbf{Tapilot-Crossing}~\cite{tapilot2024} provides 1,024 human-machine interactions for interactive data analysis, testing multi-turn reasoning and visualization. \textbf{InfiAgent-DABench}~\cite{dabench2024} offers 311 questions across 55 datasets for data analysis scenarios.

\textbf{InsightBench}~\cite{insightbench2024} evaluates 100 business analytics tasks requiring insight generation---the closest to our target domain. \textbf{DS-1000}~\cite{ds1000_2022} benchmarks data science code generation across NumPy, Pandas, and other libraries with 1,000 problems.

\textit{Gap:} Data analytics benchmarks focus on insight generation or code correctness, not full-stack application deployment with Databricks integration.

\subsection{Interactive Agent Benchmarks}

\textbf{WebArena}~\cite{zhou2024webarena} evaluates 812 web tasks across e-commerce, forums, and content management, where best agents achieve 61.7\% versus 78\% human performance. \textbf{GAIA}~\cite{mialon2023gaia} tests general AI assistants on 466 multi-step reasoning questions requiring tool use, with agents reaching 80.7\% versus 92\% human baseline.

\textbf{AgentBench}~\cite{agentbench2023} spans 8 environments (OS, databases, web) revealing significant gaps between commercial and open-source models. \textbf{OSWorld}\footnote{\url{https://os-world.github.io/}} (NeurIPS 2024) benchmarks multimodal agents in real computer environments, where recent advances have achieved superhuman performance (76\% vs 72\% human baseline).

\textit{Gap:} Interactive benchmarks evaluate general agent capabilities, not software deployment pipelines.

\subsection{Agent Evaluation Harnesses and Frameworks}

\textbf{Inspect AI}\footnote{\url{https://inspect.aisi.org.uk/}} from the UK AI Safety Institute provides 100+ pre-built evaluations with sandboxing, MCP tool support, and multi-agent primitives. It has been adopted by frontier labs and safety organizations for standardized agent evaluation.

\textbf{DeepEval}\footnote{\url{https://github.com/confident-ai/deepeval}} offers CI/CD integration with LLM-as-judge metrics including task completion, tool correctness, and hallucination detection. \textbf{Databricks Agent Evaluation} integrates with MLflow for tracking groundedness, correctness, and coherence of agentic applications.

The emerging \textbf{AgentOps} paradigm extends DevOps principles to AI agents, addressing observability, tracing, and lifecycle management specific to autonomous systems.

\subsection{DevOps and Deployment Metrics}

\textbf{DORA metrics}~\cite{forsgren2018accelerate}---deployment frequency, lead time, change failure rate, and mean time to restore---provide industry-standard measures of software delivery performance. \textbf{MLOps 2.0} architectures integrate CI/CD with Continuous Data Validation (CDV) for reliable ML delivery.

\textit{Gap:} No existing framework combines code generation evaluation with DORA-mapped deployment metrics and agentic DevX scores (runability, deployability).

\begin{table}[t]
\caption{Comparison of agent evaluation approaches. Klaudbiusz uniquely combines deployment-centric metrics with DORA mapping and trajectory-based optimization.}
\label{tab:related-comparison}
\centering
\small
\begin{tabular}{lccccc}
\toprule
Framework & Code Gen & Deploy & DORA & DevX & Trajectory \\
\midrule
HumanEval/MBPP & \checkmark & -- & -- & -- & -- \\
SWE-bench & \checkmark & -- & -- & -- & -- \\
WebArena/GAIA & -- & -- & -- & -- & -- \\
Inspect AI & \checkmark & -- & -- & -- & -- \\
DeepEval & \checkmark & -- & -- & -- & -- \\
\textbf{Klaudbiusz (Ours)} & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\
\bottomrule
\end{tabular}
\end{table}

%%
%% Installable Domain Knowledge
%%
\section{Installable Domain Knowledge}
\label{sec:domain-knowledge}

\subsection{Motivation}

Developers already use capable agents---Claude Code, Cursor, Codex. Rather than asking them to adopt yet another tool, we bring domain knowledge to the agents they already use. A user installs our package into their existing environment; the agent gains domain expertise without workflow changes. However, capable agents often spend excessive steps on fixing issues when domain-specific guidance is missing.

This approach has a practical advantage: we leverage the ecosystem of existing agents rather than competing with them. The alternative---building a custom agent per domain---doesn't scale and, in our view, creates vendor lock-in.

\textbf{Scope:} Our current implementation targets data-centric web applications on Databricks, validating the approach on a concrete domain before generalizing.

\subsection{Architecture}

The package has three components: context layers that inject domain knowledge progressively, tools exposed via CLI, and a state machine that enforces validation before deployment.

\subsubsection{Context Layers (Injected Progressively)}

Agents have limited context windows. Dumping all domain knowledge upfront wastes tokens and can confuse the model. Instead, we inject context progressively---each layer activates when relevant. This is especially important when context is assembled from multiple parts.

\begin{table}[h]
\caption{Context layer injection strategy}
\label{tab:context-layers}
\centering
\small
\begin{tabular}{llp{3.5cm}}
\toprule
\textbf{Layer} & \textbf{Content} & \textbf{When Injected} \\
\midrule
L0: Tools & Tool names/descriptions & Always (protocol-level) \\
L1: Workflow & Patterns, CLI usage, validation rules & On first discovery call \\
L2: Target & App vs job vs pipeline constraints & When target type detected \\
L3: Template & SDK patterns, examples & After scaffolding or from CLAUDE.md \\
\bottomrule
\end{tabular}
\end{table}

For example, an agent scaffolding a new app receives L0--L2 initially. Only after scaffolding completes does L3 activate---providing SDK-specific patterns like ``how to draw charts with Recharts'' or ``tRPC router conventions''. For existing projects, the agent reads CLAUDE.md (placed in the project root) to acquire L3 context.

\textbf{Implementation:} Context layers are implemented as \texttt{.mdc} rule files in \texttt{.cursor/rules/} directories. Each file specifies glob patterns for activation and contains domain-specific guidance. For example, \texttt{database-queries.mdc} provides Drizzle ORM patterns:

\begin{lstlisting}[language=bash,basicstyle=\ttfamily\footnotesize]
# database-queries.mdc (excerpt)
- Use proper Drizzle operators: eq(), gte(),
  desc() from 'drizzle-orm'
- Never use direct comparisons like
  table.column === value
- For conditional queries, build step-by-step:
  let query = db.select().from(table);
  const conditions: SQL<unknown>[] = [];
  if (filter) {
    conditions.push(eq(table.field, filter));
  }
  query = query.where(and(...conditions));
\end{lstlisting}

These rules are \emph{installable}---users add them to their project, and compatible agents (Cursor, Claude Code) automatically load them based on file patterns. Lines of code for the complete scaffolding: $\sim$2,400 across 14 rule files.

\subsubsection{Tools (Exposed via CLI)}

We expose domain functionality through CLI commands---a pattern that Cloudflare (``Code Mode'')~\cite{cloudflare2024codemode} and Anthropic~\cite{anthropic2024tools} found effective, reporting that LLMs perform better writing code to call CLI tools than invoking tools directly.

\textbf{Lifecycle commands:} \texttt{scaffold} (creates project from template with CLAUDE.md guidance), \texttt{validate} (builds in Docker, captures Playwright screenshots), \texttt{deploy} (to target platform).

\textbf{Data exploration:} Commands for discovering available data, with agent-friendly additions: batch operations that bundle multiple queries, clearer error messages, and syntax examples for platform-specific SQL variations.

Workspace tools (read/write/edit, grep, glob, bash) are not our contribution---agents already have these. Our package adds domain-specific capabilities on top.

\subsubsection{State Machine}

In our design, applications cannot deploy unless they pass validation after their most recent modification:

\begin{center}
\texttt{Scaffolded $\xrightarrow{\text{edit}}$ Modified $\xrightarrow{\text{validate}}$ Validated(checksum) $\xrightarrow{\text{deploy}}$ Deployed}
\end{center}

\textbf{How the checksum works:} The checksum captures file state at validation time using MD5 hashes of critical files (e.g., \texttt{App.tsx}, \texttt{schema.ts}). Any change after validation requires re-validation. This prevents untested code deployment---a common failure mode when agents skip validation. Implementation: \texttt{check\_template\_failed()} in our analysis code computes \texttt{hashlib.md5(content).hexdigest()} and compares against known template hashes to detect generation failures.

\subsection{Agent Compatibility}

To validate agent-agnosticism, we tested the package across multiple backends. The key requirement is function calling capability---any agent that can invoke tools works with our package.

\begin{table}[h]
\caption{Agent backend compatibility validation}
\label{tab:agent-compat}
\centering
\small
\begin{tabular}{lll}
\toprule
\textbf{Backend} & \textbf{Validation} & \textbf{Notes} \\
\midrule
Claude Agent SDK & Automated & Primary production use \\
Cursor & Manual & IDE integration \\
Codex & Manual & Alternative agent \\
LiteLLM + Qwen3 & Automated & Open-source models \\
\bottomrule
\end{tabular}
\end{table}

The LiteLLM backend demonstrates that the approach isn't tied to specific vendors---we wrap any model with function calling into our generation pipeline. Production validation with Qwen3-Coder-480B achieved 70\% success rate at \$0.61/app (vs 86.7\% at \$5.01/app for Claude Sonnet 4).

%%
%% The Klaudbiusz Framework
%%
\section{The Klaudbiusz Toolset}
\label{sec:framework}

\subsection{Agent-Agnostic Design}

Klaudbiusz provides reusable infrastructure that any agentic coding system can leverage, rather than a specific agent implementation. The toolset comprises:

\begin{itemize}
    \item \textbf{Environment Scaffolding.} Pre-configured TypeScript + tRPC project templates with Databricks SDK integration, ensuring consistent structure across generated applications. Templates are \emph{pluggable}---users can swap template sets for different stacks.
    \item \textbf{MCP Tool Integrations.} Model Context Protocol tools for file operations, database queries, and deployment actions that agents can invoke.
    \item \textbf{Containerized Execution.} Dagger-based sandboxed builds providing isolation and reproducibility across environments.
    \item \textbf{Validation Pipelines.} Automated checks for build, runtime, type safety, and deployment readiness.
\end{itemize}

\subsection{Evaluation Design Principles}

Our evaluation framework is built on two core principles:

\textbf{Zero-Bias Metrics.} All metrics are objective, reproducible, and automatable. We explicitly exclude subjective assessments of code quality, maintainability, or aesthetics.

\textbf{Tooling-Centric Feedback.} When agents fail, we ask ``what tooling improvement would help?'' rather than ``what's wrong with the agent?'' This framing drives continuous infrastructure improvement.

\subsection{The 13-Metric Rubric}

We organize our metrics into four categories spanning core functionality, platform integration, agentic DevX, and generation efficiency.

\subsubsection{Core Functionality (L1--L4, Binary)}

\begin{itemize}
    \item \textbf{L1: Build Success.} Project compiles; \texttt{docker build} exits with code 0.
    \item \textbf{L2: Runtime Success.} App starts and serves content; health check responds within 30s.
    \item \textbf{L3: Type Safety.} \texttt{npx tsc --noEmit} passes with zero errors.
    \item \textbf{L4: Tests Pass.} Unit/integration tests pass with coverage $\geq$70\%.
\end{itemize}

\subsubsection{Platform Integration (L5--L7, Binary)}

\begin{itemize}
    \item \textbf{L5: DB Connectivity.} Databricks connection works; queries execute without errors.
    \item \textbf{L6: Data Operations.} CRUD operations return correct data from tRPC procedures.
    \item \textbf{L7: UI Validation.} Frontend renders without errors (VLM verification).
\end{itemize}

\subsubsection{Agentic DevX (D8--D9, 0--5 Score)}
\label{sec:devx}

These metrics capture what agents need but humans can work around. Consider an agent that generates a working application. A human developer can run it: they'll figure out missing environment variables, install unlisted dependencies, work around unclear documentation. An agent \emph{can sometimes} do this too, but it's slow and inefficient---spending many turns on trial-and-error that explicit configuration would eliminate. The agent needs explicit \texttt{.env.example} files, documented commands, and health endpoints for verification.

\begin{itemize}
    \item \textbf{D8: Runability.} Can a sample AI agent run generated apps locally?
    \begin{itemize}
        \item 0: install/start fails; missing scripts/env
        \item 1--2: starts with manual tweaks
        \item 3: starts cleanly with .env.example + documented steps
        \item 4: starts with seeds/migrations via scripts
        \item 5: + healthcheck endpoint + smoke test succeeds
    \end{itemize}
    \item \textbf{D9: Deployability.} Can a sample AI agent deploy a generated app?
    \begin{itemize}
        \item 0: no/broken Dockerfile
        \item 1--2: image builds; container fails or healthcheck fails
        \item 3: healthcheck OK; smoke 2xx
        \item 4: + logs/metrics hooks present
        \item 5: + automated rollback to prior known-good tag
    \end{itemize}
\end{itemize}

\textbf{Why This Matters.} Existing metrics miss this distinction. Build success (binary) doesn't capture whether the build process is agent-friendly. We need metrics that ask: can another agent operate this output? This is a novel evaluation dimension absent from existing benchmarks---critical for compound AI systems where one agent's output becomes another's input.

\textbf{Implementation.} D8 and D9 are currently evaluated as \emph{artifact-based proxy metrics}: checklist-style checks for the presence and correctness of artifacts\footnote{\url{https://github.com/neondatabase/appdotbuild-agent/blob/main/klaudbiusz/cli/evaluation/evaluate_app.py\#L590-L740}} (README with setup instructions, \texttt{.env.example}, \texttt{npm start} script, Dockerfile, multi-stage build, \texttt{HEALTHCHECK}, absence of hardcoded secrets). These are necessary but not sufficient conditions---a README can exist but be incorrect.

To address this gap, we plan to augment proxy checks with \emph{statistical agent simulation}: running the same task (install, start, deploy) with multiple eval agents configured with diverse developer personas (junior frontend, senior backend, data scientist, AI agent with different capability levels). The score becomes the success rate across persona-runs (e.g., 4/5 succeed $\rightarrow$ 0.8). Variance across personas indicates documentation quality: if only the senior backend persona succeeds, the setup instructions are insufficient. At Haiku-level costs (\textasciitilde\$0.001/attempt), running 15 attempts per app (5 personas $\times$ 3 runs) costs \$0.015/app---negligible compared to generation costs.

\subsubsection{Efficiency Metrics (E10--E13, Numeric)}

\begin{itemize}
    \item \textbf{E10: Tokens Used.} Total tokens (prompt + completion) for generation.
    \item \textbf{E11: Generation Time.} Time spent generating application (seconds).
    \item \textbf{E12: Agent Turns.} Number of conversation turns during generation.
    \item \textbf{E13: LOC.} Lines of code in generated application.
\end{itemize}

\subsubsection{SQL Quality Pillar (S1--S4)}

Inspired by BIRD~\cite{bird2023} and Spider~\cite{yu2018spider}, we evaluate SQL quality \emph{in situ}---within generated full-stack applications rather than against isolated query benchmarks. SQL queries are extracted from generated code\footnote{\url{https://github.com/neondatabase/appdotbuild-agent/blob/main/klaudbiusz/cli/evaluation/eval_checks.py\#L150-L190}} (inline from TypeScript for tRPC apps, or from \texttt{config/queries/*.sql} for DBX SDK apps) and evaluated:

\begin{itemize}
    \item \textbf{S1: Execution Correctness (EX).} Fraction of extracted SQL queries that execute without error against Databricks and return non-empty results. Range: $[0,1]$.
    \item \textbf{S2: Valid Efficiency Score (VES).} Adapted from BIRD~\cite{bird2023}: rewards both correctness \emph{and} query efficiency relative to a reference solution. $\text{VES} = \text{correctness} \times \min(1, t_{\text{ref}} / t_{\text{actual}})$. Range: $[0,1]$.
    \item \textbf{S3: Query Complexity.} Distribution across difficulty tiers (easy/medium/hard/extra-hard) based on SQL AST features (JOINs, subqueries, window functions) via \texttt{sqlglot}. Reported as context, not scored.
    \item \textbf{S4: SQL Safety.} Static analysis for destructive operations (\texttt{DROP}, \texttt{TRUNCATE}, \texttt{DELETE} without \texttt{WHERE}), parameterization, and injection resistance. Range: $[0,1]$.
\end{itemize}

The key distinction from existing text-to-SQL benchmarks is that S1--S4 evaluate queries in their deployment context---connected to real Databricks schemas, embedded in application logic, and subject to runtime constraints. The extraction infrastructure is implemented; automated scoring against Databricks is planned for the 100-prompt benchmark.

\subsubsection{Web Quality Pillar (W1--W4)}

Inspired by WebArena~\cite{zhou2024webarena} and VisualWebArena~\cite{visualwebarena2024}, we design a \emph{use-case-based} web evaluation framework with accessibility-tagged acceptance criteria:

\begin{itemize}
    \item \textbf{W1: Task Completion Rate.} Per-prompt acceptance criteria generated as testable user stories (e.g., for a ``customer churn dashboard'': can the user filter by date range? does the table show customer data?). Evaluated via Playwright test scripts. Range: $[0,1]$.
    \item \textbf{W2: Visual Rendering Correctness.} Extends our current binary VLM check\footnote{\url{https://github.com/neondatabase/appdotbuild-agent/blob/main/klaudbiusz/cli/evaluation/evaluate_app.py\#L509-L588}} to a structured multi-aspect rubric: charts render with labels, layout is responsive, no overlapping elements, text is readable. Range: $[0,1]$.
    \item \textbf{W3: Interactive Element Functionality.} Playwright-based interaction testing: click buttons, fill forms, toggle filters, verify DOM changes and network requests. Range: $[0,1]$.
    \item \textbf{W4: Accessibility Score.} WCAG 2.1 AA compliance via \texttt{axe-core} integration with Playwright. Measures: keyboard navigation, ARIA labels, contrast ratios, focus indicators. Score: $1 - (v_{\text{critical}} / n_{\text{elements}})$, clamped to $[0,1]$.
\end{itemize}

Acceptance criteria map to WCAG categories (Perceivable, Operable, Understandable, Robust), enabling systematic accessibility tracking across generated applications. The current implementation provides binary UI validation (L7); the full W pillar is designed for the 100-prompt benchmark expansion.

\subsection{AppEval-100 Composite Score}

To enable automatic, comparable measurement across prompts and runs, we introduce \textbf{AppEval-100}---a single numeric index representing normalized readiness and agentic operability on a 0--100 scale. Unlike existing benchmarks that evaluate SQL correctness (BIRD/Spider) or web task completion (WebArena) in isolation, AppEval-100 provides the first composite evaluation combining all four pillars for full-stack data applications.

\textbf{Step 1: Reliability Pillar (R).} Aggregate core runtime checks:
\[
R = \text{GM}(b_{\text{build}}, b_{\text{runtime}}, b_{\text{type}}, b_{\text{tests}})
\]

\textbf{Step 2: SQL Quality Pillar (S).} Weighted combination of SQL metrics:
\[
S = 0.50 \times S_1 + 0.30 \times S_2 + 0.20 \times S_4
\]
where $S_1$ (execution correctness) dominates, $S_2$ (efficiency) contributes secondary value, and $S_4$ (safety) ensures secure queries.

\textbf{Step 3: Web Quality Pillar (W).} Weighted combination of UI/UX metrics:
\[
W = 0.40 \times W_1 + 0.30 \times W_2 + 0.20 \times W_3 + 0.10 \times W_4
\]
prioritizing task completion and visual correctness.

\textbf{Step 4: Agentic DevX Pillar (D).}
\[
D = \text{GM}(\hat{x}_{\text{run}}, \hat{x}_{\text{deploy}})
\]
where $\hat{x} = \text{score}/5$ normalizes 0--5 scores to $[0,1]$.

\textbf{Step 5: Soft Penalty Gate.} Penalize critical outages without collapsing to zero:
\[
G = (0.25 + 0.75 \times b_{\text{build}}) \times (0.25 + 0.75 \times b_{\text{runtime}}) \times (0.50 + 0.50 \times b_{\text{db}}) \times (0.50 + 0.50 \times \mathbf{1}_{S_1 \geq 0.5})
\]

\textbf{Step 6: Final Composite.}
\[
\textbf{AppEval-100} = 100 \times (0.30 \times R + 0.25 \times S + 0.25 \times W + 0.20 \times D) \times G
\]

\textbf{Pillar Weight Rationale:} Reliability (30\%) ensures the app works; SQL Quality (25\%) and Web Quality (25\%) capture the core value proposition for Databricks data applications; DevX (20\%) measures autonomous operability.

Values near 100 denote near-perfect readiness; 50--70 indicates partial operability; $<$30 signifies fundamental execution issues.

\subsection{Cluster-Based Quality Assessment}
\label{sec:cluster-eval}

The pillar weights in AppEval-100 (0.30/0.25/0.25/0.20) are manually assigned, which raises a natural question: \emph{why these weights?} We propose a data-driven alternative inspired by metric-based clustering in software engineering~\cite{knyazev2009clustering}.

\subsubsection{Metric Vector Representation}

Each generated application is represented as a $p$-dimensional metric vector:
\[
\mathbf{v}_{\text{app}} = \langle L_1, L_2, \ldots, L_7, \hat{D}_8, \hat{D}_9, S_1, \ldots, S_4, W_1, \ldots, W_4, \tilde{E}_{10}, \ldots, \tilde{E}_{13} \rangle
\]
where $\hat{D}$ denotes normalized DevX scores ($\text{score}/5$) and $\tilde{E}$ denotes min-max normalized efficiency metrics. This yields up to 21 dimensions encompassing all four pillars plus efficiency.

\subsubsection{Cosine Similarity Clustering}

We apply $k$-means clustering with cosine distance rather than Euclidean distance:
\[
\rho(\mathbf{v}_1, \mathbf{v}_2) = 1 - \frac{\mathbf{v}_1 \cdot \mathbf{v}_2}{\|\mathbf{v}_1\| \, \|\mathbf{v}_2\|}
\]

Cosine distance eliminates scale dependence: an application with 10$\times$ more lines of code is not penalized differently than a compact one. What matters is the \emph{shape} of the metric profile---the relative strengths across dimensions---not the absolute magnitude. This property was shown to be critical for clustering software metrics where individual metric scales vary by orders of magnitude~\cite{knyazev2009clustering}.

\subsubsection{Quality Tier Assignment}

We cluster applications into $k$ quality tiers (e.g., $k=3$: Production-Ready, Functional, Broken). An expert labels each cluster centroid once; subsequent applications are classified by nearest centroid. This provides:

\begin{itemize}
    \item \textbf{No manual weights.} Quality tiers emerge from the data structure, not from predefined formulas.
    \item \textbf{Graceful handling of missing metrics.} When S or W pillar metrics are unavailable (as in our current evaluation), clustering operates on the available dimensions without formula breakdown.
    \item \textbf{Scalability.} Adding new metrics (e.g., security checks, performance benchmarks) requires no weight re-tuning---they simply extend the vector.
\end{itemize}

\subsubsection{Cluster Validation}

Following~\cite{knyazev2009clustering}, we validate cluster quality using purity $P_Q$ and entropy $E_Q$:
\[
P_Q = \frac{1}{N} \sum_{j=1}^{k} \max_{i} |q_j \cap c_i|, \qquad
E_Q = \sum_{j=1}^{k} \frac{|q_j|}{N} \cdot E(q_j)
\]
where $q_j$ are clusters, $c_i$ are expert-assigned quality classes, and $E(q_j)$ is the entropy of class distribution within cluster $q_j$. High purity indicates clean separation between quality tiers.

\subsubsection{Deriving Empirical Weights}

When a single numeric score is still needed (e.g., for CI/CD gates), we derive weights empirically: apply linear discriminant analysis to the expert-labeled clusters and use the resulting coefficients as pillar weights. This yields a formula structurally identical to AppEval-100 but with \emph{data-justified} coefficients rather than manual ones.

\textbf{Current Status.} With $n=20$ applications, our dataset is insufficient for robust clustering ($k=3$ requires $\geq$30 samples per cluster for stable centroids). We report AppEval-100 with manual weights for this paper and plan to apply cluster-based scoring on the full 100-prompt benchmark, where statistical power is adequate. Preliminary analysis on the Simple 20 set shows two natural groupings (fully functional vs. type-safety failures), consistent with the L3 metric distribution in Table~\ref{tab:results}.

\subsection{DORA Metrics Mapping}

We map our metrics to industry-standard DORA~\cite{forsgren2018accelerate} measures:

\begin{itemize}
    \item \textbf{Deployment Frequency:} Count of successful D9 events per app per evaluation cohort.
    \item \textbf{Lead Time:} Median time from first model call to successful D9 deployment.
    \item \textbf{Change Failure Rate:} Fraction of deployments that fail healthcheck or rollback within 30 min.
    \item \textbf{MTTR:} Median time from failure detection to restore (prior healthy image running).
\end{itemize}

\textbf{Production Gate:} L1--L7 pass, D8$\geq$4, D9$\geq$4, type-safety pass, and DORA guardrails (Lead Time P50 $\leq$10 min, CFR $\leq$15\%, MTTR $\leq$15 min).

\textbf{Statistical DevX and DORA.} With statistical agent simulation (Section~\ref{sec:devx}), DORA metrics gain direct empirical grounding: Change Failure Rate becomes $1 - \text{D9}_{\text{statistical}}$ (fraction of persona-runs that fail deployment), and Lead Time becomes the median time across personas from \texttt{git clone} to successful health check. Variance across personas measures predictability---a key DORA dimension.

\textbf{Validation plan.} To verify that AppEval-100 predicts real-world delivery performance, we plan to deploy generated applications to staging environments and track actual DORA metrics over 30 days, correlating with evaluation scores. Our hypothesis: AppEval-100 $> 70$ correlates with DORA ``High'' performance (deployment frequency: daily, lead time: $<$1 day, CFR: $<$15\%, MTTR: $<$1 hour).

\subsection{MLflow Integration}

We integrate with Databricks Managed MLflow for experiment tracking:
\begin{itemize}
    \item Automatic metric logging per evaluation run
    \item Trend analysis across model versions and configurations
    \item Artifact versioning for reproducibility
    \item DORA telemetry for delivery performance monitoring
\end{itemize}

%%
%% Trajectory Analyzer
%%
\section{Agentic Trajectory Analyzer}
\label{sec:trajectory}

\subsection{Role in the Feedback Loop}

To run trajectory analysis at scale, we built infrastructure for bulk app generation that saves execution traces in a structured format. In production, users work with their own agents (Cursor, Claude Code) which may not save trajectories in our format---but the analyzer works with any trace data that captures tool calls and results.

The analyzer consumes trajectories and recommends package improvements. This closes the loop: agents struggle $\rightarrow$ we see it in trajectories $\rightarrow$ we fix the tooling $\rightarrow$ agents struggle less.

\subsection{Why Trajectories, Not Just Outcomes}

End-state metrics (build success, test pass) don't reveal causes:
\begin{itemize}
    \item Model limitations (reasoning, instruction following)?
    \item Tool problems (unclear descriptions, missing functionality)?
    \item Template issues (incorrect scaffolding, missing guidance)?
    \item Prompt issues (underspecified requirements, contradicting constraints)?
\end{itemize}

Trajectories---the sequence of reasoning, tool calls, and results---show where things went wrong. \textbf{Example:} An agent retrying the same malformed SQL five times reveals a missing example in the guidance. An agent calling N tools for N tables reveals a missing batch operation.

\subsection{Two-Phase Architecture}

We employ a map-reduce approach optimized for cost and quality:

\textbf{Map Phase (Cheap Model).} Each trajectory is processed independently by a fast model (we use Claude Haiku, $\sim$\$0.001/trajectory), extracting:
\begin{itemize}
    \item Errors and retries
    \item Confusion patterns (agent asking clarifying questions)
    \item Inefficiency (suboptimal tool sequences)
    \item \textbf{Repetitions} (same action attempted multiple times)
\end{itemize}
This runs in parallel across all trajectories.

\textbf{Agentic Synthesis Phase (Reasoning Model).} Aggregated patterns go to a reasoning model (we use Claude Opus) with read-only access to:
\begin{itemize}
    \item Template and CLI tools source code (via Read/Glob/Grep)
    \item Tool definitions (extracted from MCP server)
    \item Evaluation metrics (per-app scores, optional)
\end{itemize}

This is a full agent with up to 50 turns of exploration. If trajectories show SQL confusion, the agent greps templates for SQL examples. If tool descriptions seem unclear, it reads implementations. Context is discovered progressively as patterns demand.

\textbf{Extensibility.} The architecture naturally extends to new context sources. We started with trajectories only, then added template source code access, then tool definitions extracted via the MCP binary. Adding new sources (e.g., user feedback, production logs) requires only pointing the synthesis agent at additional files.

\textbf{Example trace through synthesis:} Given 20 trajectories where agents repeatedly fail on \texttt{QUALIFY} syntax:
\begin{enumerate}
    \item Map phase extracts: ``SQL error on QUALIFY clause'' (15 occurrences)
    \item Synthesis agent searches: \texttt{grep -r "QUALIFY" templates/}
    \item Finds: no QUALIFY examples in SQL guidance
    \item Recommendation: ``Add QUALIFY, PIVOT syntax to SQL guidance''
\end{enumerate}

\subsection{Concrete Improvements}

The analyzer identified issues leading to fixes we implemented:

\begin{table}[h]
\caption{Trajectory-identified improvements (implemented)}
\label{tab:improvements}
\centering
\small
\begin{tabular}{p{2.2cm}p{2.2cm}p{2.8cm}}
\toprule
\textbf{Pattern Observed} & \textbf{Diagnosis} & \textbf{Fix Applied} \\
\midrule
N separate calls for N tables & Missing batch operation & Added \texttt{discover\_schema} batch command \\
Agents expecting list, got search & Confusing tool name & Renamed \texttt{list\_tables} $\rightarrow$ \texttt{find\_tables} \\
Repeated SQL syntax errors & Missing examples & Added QUALIFY, PIVOT syntax to guidance \\
Retries on malformed errors & Unclear error messages & Added contextual parameter messages \\
\bottomrule
\end{tabular}
\end{table}

These aren't hypothetical---they're actual fixes derived from trajectory analysis and committed to the codebase. Evidence: commit history in \texttt{github.com/neondatabase/appdotbuild-agent}.

\subsection{Cost Model}

For N trajectories:
\begin{itemize}
    \item Map: N $\times$ $\sim$\$0.001 (cheap model)
    \item Synthesis: 1 $\times$ $\sim$\$0.5--3 (reasoning model, bounded at 50 turns)
\end{itemize}

Total scales linearly but remains bounded. For 20 apps, analysis cost was under \$15.

\subsection{Future Direction}

Our current approach is semi-automatic: the analyzer outputs recommendations, but a human reviews them and decides which to implement. This keeps a human in the loop for changes to production tooling.

Recent work on reflective prompt evolution (GEPA~\cite{gepa2024}) shows prompts can be automatically optimized through self-reflection. DSPy~\cite{khattab2023dspy} demonstrates similar techniques for prompt tuning. These techniques could close this gap---automatically applying fixes, measuring improvement, and iterating without human intervention. The analyzer's recommendations target tool descriptions, prompts, and examples---artifacts amenable to such techniques.

\subsection{Feedback Loop}

The trajectory analyzer enables a continuous improvement cycle with optional regression testing:

\begin{center}
Generate $\rightarrow$ Evaluate $\rightarrow$ Analyze Trajectories $\rightarrow$ Improve Scaffolding $\rightarrow$ Regression Suite $\rightarrow$ Repeat
\end{center}

%%
%% Experimental Setup
%%
\section{Experimental Setup}
\label{sec:setup}

\subsection{AppEval-100 Benchmark Design}

We design a 100-prompt benchmark targeting statistical validity for Databricks data application evaluation. Our sample size provides $\pm$9\% confidence interval at 95\% confidence for binary metrics ($p=0.7$), matching InsightBench~\cite{insightbench2024} (100 tasks) and exceeding Spider 2.0's~\cite{spider2_2024} focused enterprise subset (632 tasks).

\subsubsection{Difficulty Distribution}

\begin{table}[h]
\caption{Prompt difficulty distribution ($n=100$)}
\label{tab:difficulty}
\centering
\begin{tabular}{lcp{5cm}}
\toprule
Tier & Count & Description \\
\midrule
Simple & 40 & Single-entity CRUD, basic dashboards, one data source \\
Medium & 40 & Multi-entity JOINs, filters, interactive charts, 2--3 data sources \\
Hard & 20 & Complex analytics, multi-step workflows, real-time updates \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Domain Coverage}

Prompts span five application domains: Analytics Dashboards (26\%), CRUD Applications (22\%), Data Visualization (22\%), Business Intelligence (20\%), and Reporting Tools (10\%).

\subsubsection{Schema Families}

We target six schema families to ensure diversity: TPC-DS (25 prompts, retail/customer analytics), TPC-H (20, supply chain), NYC Taxi (15, trip analytics), Custom Databricks (25, Unity Catalog/ML features), Financial (10, trading/risk), and IoT/Telemetry (5, device metrics).

\subsection{Prompt Collection Methodology}

Prompts are collected from three sources to ensure realistic ambiguity and domain vocabulary:

\begin{enumerate}
    \item \textbf{Hackathon recordings} (50 prompts): Extracted from video transcripts of internal Databricks application hackathons, preserving natural language vagueness.
    \item \textbf{Production logs} (30 prompts): Anonymized user requests from app.build, capturing real-world requirements.
    \item \textbf{Synthetic generation} (20 prompts): LLM-generated from templates to fill coverage gaps.
\end{enumerate}

\textbf{Quality Criteria:} Prompts must exhibit realistic ambiguity, domain-specific vocabulary, implicit requirements (unstated but expected features), and achievability with current templates.

\subsection{Current Evaluation Dataset}

For this paper, we evaluate on a ``Simple 20'' subset---20 Databricks data application prompts spanning dashboards, analytics, and business intelligence tools---retained as a fixed regression set for longitudinal comparison.

\subsection{Generation Pipeline}

Applications are generated using:
\begin{itemize}
    \item \textbf{Claude Agent SDK} with edda MCP for tool orchestration
    \item \textbf{Dagger} containerized execution for isolation and reproducibility
    \item \textbf{Environment scaffolding} with templates and validation pipelines
\end{itemize}

\subsection{Generation--Evaluation Isolation}
\label{sec:isolation}

A critical design requirement is that the generator and evaluator must be \emph{strictly isolated}. Without isolation, the generator agent can exploit evaluation artifacts---a form of reward hacking~\cite{amodei2016concrete} that inflates metrics without improving application quality. We describe the threat model, our mitigation, and implications for the field.

\subsubsection{Threat Model: Metric Gaming Through Code Leakage}

Agentic coding systems actively explore their filesystem via tools like \texttt{Read}, \texttt{Glob}, and \texttt{Grep}. When evaluation code resides in the same workspace as generation code, the following attack surfaces emerge:\footnote{Examples are hypothetical threat scenarios that motivated our isolation design. We observed agents reading non-generation files during early development, which led to the container-level isolation described in Section~\ref{sec:container-isolation}.}

\textbf{Scenario 1: Eval check reverse-engineering.} Our DB connectivity check (\texttt{eval\_checks.py:39--70}\footnote{\url{https://github.com/neondatabase/appdotbuild-agent/blob/main/klaudbiusz/cli/evaluation/eval_checks.py\#L39-L70}}) verifies connectivity by calling tRPC endpoints and parsing JSON responses for a \texttt{result} key. An agent with read access could discover this check and create minimal stubs that pass the binary metric without implementing genuine Databricks queries:

\begin{verbatim}
// Hypothetical stub (passes L5 check, no real data)
healthcheck: publicProcedure.query(() => ({
  result: { data: [], status: "ok" }
}))
\end{verbatim}

\textbf{Scenario 2: Composite score optimization.} The AppEval-100 formula and pillar weights are defined in \texttt{eval\_metrics.py:44--88}\footnote{\url{https://github.com/neondatabase/appdotbuild-agent/blob/main/klaudbiusz/cli/evaluation/eval_metrics.py\#L44-L88}}. An agent that reads this file learns that Reliability (30\%) and DevX (20\%) are weighted separately, and could prioritize boilerplate (\texttt{.env.example}, \texttt{Dockerfile}) for DevX points while neglecting SQL quality metrics that require genuine implementation.

\textbf{Scenario 3: Pattern copying from prior runs.} If the \texttt{app/} directory from prior generations is accessible, an agent could copy file structures verbatim from successful runs rather than reasoning about current prompt requirements---producing high-scoring but homogeneous applications that fail on novel prompts.

\textbf{Scenario 4: Trajectory-informed shortcuts.} With access to prior trajectory logs, an agent could identify which tool sequences led to passing evaluations and replay them mechanically, bypassing exploratory reasoning.

\subsubsection{Container-Level Isolation via Dagger}
\label{sec:container-isolation}

We enforce isolation at the container build level. The generation Dockerfile\footnote{\url{https://github.com/neondatabase/appdotbuild-agent/blob/main/klaudbiusz/Dockerfile\#L58-L64}} selectively copies only generation-related code, explicitly excluding evaluation:

\begin{verbatim}
# Exclude evaluation to prevent reward hacking
COPY cli/generation/    ./cli/generation/
COPY cli/utils/         ./cli/utils/
# NOT copied: cli/evaluation/,
#             cli/analyze_trajectories.py
\end{verbatim}

The Dagger build context further excludes runtime artifacts\footnote{\url{https://github.com/neondatabase/appdotbuild-agent/blob/main/klaudbiusz/cli/generation/dagger_run.py\#L206-L212}}:

\begin{verbatim}
context = client.host().directory(".",
    exclude=["app/", "app-eval/",
             "results/", ".venv/", ".git/"])
\end{verbatim}

Each generation starts from a clean workspace. In bulk runs, a pre-generation directory snapshot\footnote{\url{https://github.com/neondatabase/appdotbuild-agent/blob/main/klaudbiusz/cli/generation/container_runner.py\#L76}} prevents cross-contamination between parallel generations.

\begin{table}[h]
\caption{Isolation boundaries between generation and evaluation.}
\label{tab:isolation}
\centering
\small
\begin{tabular}{lcc}
\toprule
Artifact & Generator & Evaluator \\
\midrule
Domain package (.mdc rules) & \checkmark & --- \\
MCP tools (Databricks CLI) & \checkmark & --- \\
Installed skills & \checkmark & --- \\
Evaluation checks / metrics & --- & \checkmark \\
Prior generated apps / results & --- & \checkmark \\
Trajectory analyzer & --- & \checkmark \\
Current app code & writes & reads \\
Current trajectory & writes & reads \\
\bottomrule
\end{tabular}
\end{table}

This separation mirrors training/test splits in machine learning: the generator learns from the domain package (training signal), while the evaluator measures generalization against unseen criteria. Improvements flow exclusively through the domain package---not through gaming the evaluation protocol.

\subsubsection{Implications for Agent Evaluation}

Our isolation design addresses a class of vulnerabilities unique to agentic systems: (1) agents actively explore their filesystem and will read evaluation code if accessible; (2) binary metrics are especially vulnerable to trivial stubs that pass without useful output; (3) composite score formulas, when known, enable strategic effort allocation that maximizes score over quality; (4) access to prior outputs enables pattern matching instead of reasoning.

We recommend that agent evaluation frameworks enforce container-level or filesystem-level isolation between generation and evaluation as a baseline requirement.

\subsection{Statistical Validity}

Our 100-prompt benchmark provides:
\begin{itemize}
    \item \textbf{Confidence Interval:} $\pm$9\% at 95\% confidence for binary metrics
    \item \textbf{Statistical Power:} 0.80 for medium effect size ($d=0.5$)
    \item \textbf{Stratification:} Three difficulty tiers adequately sampled (40/40/20)
\end{itemize}

The ``Simple 20'' regression set enables longitudinal model comparisons while guarding against prompt inflation effects.

%%
%% Results
%%
\section{Results}
\label{sec:results}

\subsection{Overall Performance}

We evaluate on the ``Simple 20'' prompt set---20 Databricks data application prompts spanning dashboards, analytics, and business intelligence tools.

\begin{table}[h]
\caption{Aggregate evaluation results ($n=20$ applications, Evals 2.0)}
\label{tab:results}
\begin{tabular}{llcc}
\toprule
ID & Metric & Result & Notes \\
\midrule
L1 & Build Success & 20/20 & 100\% pass \\
L2 & Runtime Success & 20/20 & 100\% pass \\
L3 & Type Safety & 1/20 & 5\% pass (improvement needed) \\
L4 & Tests Pass & -- & Not yet instrumented \\
\midrule
L5 & DB Connectivity & 18/20 & 90\% pass \\
L6 & Data Operations & -- & Requires app-specific procedures \\
L7 & UI Validation & -- & VLM check in progress \\
\midrule
D8 & Runability & 3.0/5 & Average score \\
D9 & Deployability & 2.5/5 & Average score \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Finding:} 100\% of generated applications achieve build and runtime success, with 90\% achieving functional Databricks connectivity. However, type safety (5\%) and agentic DevX scores (3.0/5, 2.5/5) indicate room for improvement toward production readiness.

\subsection{Generation Efficiency Metrics}

\begin{table}[h]
\caption{Efficiency metrics ($n=20$ applications)}
\label{tab:efficiency}
\begin{tabular}{lcc}
\toprule
Metric & Value & Notes \\
\midrule
E10: Total Tokens & ~16K/app & Prompt + completion \\
E11: Generation Time & 6--9 min & End-to-end \\
E12: Agent Turns & 93 avg & Conversation turns \\
E13: LOC & 732 avg & Lines of code \\
\midrule
Cost per App & \$0.74 & API cost \\
Total Cost (20 apps) & \$14.81 & -- \\
Build Step Time & 2.7s avg & Docker build \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Comparison: Evals 1.0 vs 2.0}

\begin{table}[h]
\caption{Evolution from manual (Evals 1.0) to automated (Evals 2.0) evaluation}
\label{tab:comparison}
\begin{tabular}{lcc}
\toprule
Aspect & Evals 1.0 & Evals 2.0 \\
\midrule
Viability Rate & 73\% (30 apps) & 100\% build/runtime \\
Time to Deploy & 30--60 min & 6--9 min \\
Evaluation Method & Manual rubric & Automated pipeline \\
Metrics Tracked & Binary viability & 13 metrics + AppEval-100 \\
Reproducibility & Low & Full artifact pack \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Production Readiness Assessment}

Current status: \textbf{below production threshold}. To reach Production Candidate level:
\begin{itemize}
    \item L3 Type Safety: 5\% $\rightarrow$ target $\geq$90\%
    \item D8 Runability: 3.0 $\rightarrow$ target $\geq$4
    \item D9 Deployability: 2.5 $\rightarrow$ target $\geq$4
    \item DORA guardrails: Lead Time P50 $\leq$10m, CFR $\leq$15\%, MTTR $\leq$15m
\end{itemize}

\subsection{Trajectory Optimizer Insights}

Analysis of agent trajectories revealed common friction patterns:
\begin{itemize}
    \item \textbf{SQL Syntax:} Databricks SQL variations causing query failures
    \item \textbf{Error Handling:} Missing error handling in template scaffolding
    \item \textbf{Tool Descriptions:} Unclear MCP tool descriptions leading to incorrect usage
    \item \textbf{Type Inference:} TypeScript strict mode violations in generated code
\end{itemize}

These insights feed back into template and tool improvements via the optimize $\rightarrow$ evaluate $\rightarrow$ analyze cycle.

%%
%% Discussion
%%
\section{Discussion}
\label{sec:discussion}

\subsection{Limitations}

\textbf{Platform Specificity.} Our current implementation targets Databricks applications. Extending to other platforms requires platform-specific metrics (e.g., AWS Lambda, Vercel).

\textbf{Binary Metrics.} Several metrics are binary, potentially missing nuanced quality differences. Future work could introduce continuous variants.

\textbf{Dataset Size.} Our evaluation of 20 applications provides initial validation but may not capture edge cases. Scaling to larger datasets is ongoing.

\subsection{Broader Impact}

By establishing standardized metrics for autonomous deployability, we enable:
\begin{itemize}
    \item Reproducible benchmarking of agentic code generation systems
    \item Objective comparison across different approaches
    \item Systematic improvement through trajectory-based feedback
\end{itemize}

%%
%% Conclusion
%%
\section{Conclusion}
\label{sec:conclusion}

We presented Klaudbiusz, an open-source, agent-agnostic toolset for autonomous Databricks application generation. Rather than building a specific agent, we provide reusable infrastructure---scaffolding, templates, and MCP tools---that any agentic system can leverage. Our trajectory analyzer enables continuous improvement of this toolset by identifying friction points in agent execution traces. AppEval-100 provides composite evaluation combining Reliability, SQL Quality, Web Quality, and Agentic DevX, mapped to industry-standard DORA metrics.

The path to reliable agentic code generation requires not just better models, but better tooling. We release Klaudbiusz to enable the community to build and improve agent-agnostic infrastructure systematically.

\textbf{Open Source Release.} Toolset, trajectory analyzer, and evaluation harness available at: [URL redacted for review]

%%
%% Acknowledgments (hidden for review)
%%
% \begin{acks}
% Supported by Neon and Databricks.
% \end{acks}

%%
%% Bibliography
%%
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
