%%
%% This is file for CAIS 2026 submission
%% ACM Conference on AI and Agentic Systems
%%

\documentclass[sigconf,review,anonymous]{acmart}

%% Rights management information
\setcopyright{acmlicensed}
\copyrightyear{2026}
\acmYear{2026}
\acmDOI{XXXXXXX.XXXXXXX}

%% Conference information
\acmConference[CAIS '26]{ACM Conference on AI and Agentic Systems}{May 26--29, 2026}{San Jose, CA, USA}
\acmBooktitle{Proceedings of the ACM Conference on AI and Agentic Systems (CAIS '26), May 26--29, 2026, San Jose, CA, USA}
\acmISBN{978-1-4503-XXXX-X/26/05}

%% Packages
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}

%% Code listing style
\lstset{
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  backgroundcolor=\color{gray!10}
}

%%
%% Document starts
%%
\begin{document}

%%
%% Title
%%
\title{Klaudbiusz: An Open-Source Evaluation Framework for Autonomous Databricks Application Generation with Trajectory-Based Optimization}

%%
%% Authors (anonymous for review)
%%
\author{Anonymous Authors}
\affiliation{%
  \institution{Anonymous Institution}
}
\email{anonymous@example.com}

%%
%% Abstract
%%
\begin{abstract}
We present Klaudbiusz, an open-source evaluation framework for measuring autonomous deployability of AI-generated Databricks applications. As agentic coding systems mature, the field lacks standardized, objective metrics for assessing whether generated code can be deployed without human intervention. Our framework introduces a 13-metric rubric with AppEval-100 composite score spanning core functionality (build, runtime, type safety, tests), platform integration (Databricks connectivity, data operations, UI validation), agentic DevX (runability, deployability on 0--5 scales), and generation efficiency (tokens, time, turns, LOC). We map these metrics to industry-standard DORA measures and complement evaluation with a trajectory optimizer that analyzes agent execution traces using a map-reduce LLM approach to identify friction points and generate actionable recommendations. Evaluated on 20 Databricks data applications, we achieve 100\% build/runtime success with 6--9 minute generation latency at \$0.74 per application. We release the complete framework including evaluation harness, trajectory analyzer, and MLflow integration to enable reproducible benchmarking of agentic code generation systems.
\end{abstract}

%%
%% CCS Concepts
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007.10011006.10011008.10011009.10011015</concept_id>
       <concept_desc>Software and its engineering~Software testing and debugging</concept_desc>
       <concept_significance>500</concept_significance>
   </concept>
   <concept>
       <concept_id>10010147.10010178.10010179</concept_id>
       <concept_desc>Computing methodologies~Natural language processing</concept_desc>
       <concept_significance>300</concept_significance>
   </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Software testing and debugging}
\ccsdesc[300]{Computing methodologies~Natural language processing}

%%
%% Keywords
%%
\keywords{agentic code generation, evaluation framework, autonomous deployment, Databricks, trajectory optimization}

\maketitle

%%
%% Introduction
%%
\section{Introduction}
\label{sec:intro}

The emergence of agentic coding systems---AI agents capable of autonomously generating, testing, and deploying software applications---represents a paradigm shift in software engineering. While benchmarks like HumanEval~\cite{chen2021evaluating} and SWE-bench~\cite{jimenez2024swe} have advanced our understanding of code generation quality, they focus primarily on functional correctness rather than production readiness.

\textbf{The Autonomous Deployability Gap.} We identify a critical gap in current evaluation methodologies: the absence of standardized metrics for measuring whether AI-generated code can be \emph{autonomously deployed} without human intervention. Traditional metrics assess whether code runs correctly; we argue the field needs metrics that assess whether an AI agent can complete the full deployment lifecycle.

\textbf{Core Principle.} Our work is guided by a simple axiom: \emph{If an AI agent cannot autonomously deploy its own generated code, that code is not production-ready.}

\textbf{Contributions.} We make the following contributions:

\begin{enumerate}
    \item \textbf{9-Metric Evaluation Framework.} We introduce Klaudbiusz, an evaluation framework with 9 zero-bias, objective metrics organized into three categories: core functionality, platform integration, and agentic DevX.

    \item \textbf{Trajectory Optimizer.} We present a map-reduce approach for analyzing agent execution traces to identify friction points and generate actionable recommendations for improving scaffolding and tools.

    \item \textbf{Open-Source Release.} We release the complete framework with MLflow integration, enabling reproducible benchmarking of agentic code generation systems.

    \item \textbf{Empirical Validation.} We demonstrate 90\% autonomous deployment readiness on 20 Databricks data applications with 6--9 minute generation latency.
\end{enumerate}

%%
%% Related Work
%%
\section{Related Work}
\label{sec:related}

We survey evaluation approaches across four categories: function-level benchmarks, repository-level agent benchmarks, interactive agent environments, and evaluation harnesses. Table~\ref{tab:related-comparison} summarizes key frameworks and identifies the gap our work addresses.

\subsection{Function-Level Code Generation Benchmarks}

\textbf{HumanEval}~\cite{chen2021evaluating} introduced pass@k evaluation on 164 Python functions, becoming the standard metric for code generation. \textbf{MBPP}~\cite{austin2021program} expanded to 974 problems but remains algorithm-focused. Both benchmarks are now saturated---top models achieve $>$90\% on HumanEval---raising concerns about contamination and real-world relevance.

\textbf{BigCodeBench}\footnote{\url{https://github.com/bigcode-project/bigcodebench}} addresses these limitations with diverse library calls and complex instructions, while \textbf{LiveCodeBench}\footnote{\url{https://livecodebench.github.io/}} provides dynamic, contamination-resistant evaluation. Recent work on \textbf{HumanEval Pro and MBPP Pro}~\cite{humanevalpro2024} introduces self-invoking code generation to test progressive reasoning.

\textit{Gap:} Function-level benchmarks do not evaluate deployment, integration, or operational readiness.

\subsection{Repository-Level Agent Benchmarks}

\textbf{SWE-bench}~\cite{jimenez2024swe} evaluates agents on 2,294 real GitHub issues, with \textbf{SWE-bench Verified}\footnote{\url{https://openai.com/index/introducing-swe-bench-verified/}} providing 500 human-verified samples where top models achieve $\sim$72\%. The harder \textbf{SWE-bench Pro}\footnote{\url{https://scale.com/blog/swe-bench-pro}} reveals performance drops to $\sim$23\% on production-grade issues.

\textbf{SWE-agent}~\cite{yang2024swe} demonstrates that custom agent-computer interfaces significantly enhance performance through Docker-based harnesses and tool augmentation. \textbf{REPOCOD}\footnote{\url{https://arxiv.org/abs/2410.21647}} tests complete method implementation with only 6\% resolution rate, while \textbf{DevQualityEval}\footnote{\url{https://github.com/symflower/eval-dev-quality}} evaluates multi-language software engineering tasks on private datasets to avoid contamination.

\textit{Gap:} Repository-level benchmarks focus on patch correctness, not autonomous deployment capability.

\subsection{Interactive Agent Benchmarks}

\textbf{WebArena}~\cite{zhou2024webarena} evaluates 812 web tasks across e-commerce, forums, and content management, where best agents achieve 61.7\% versus 78\% human performance. \textbf{GAIA}~\cite{mialon2023gaia} tests general AI assistants on 466 multi-step reasoning questions requiring tool use, with agents reaching 80.7\% versus 92\% human baseline.

\textbf{AgentBench}~\cite{agentbench2023} spans 8 environments (OS, databases, web) revealing significant gaps between commercial and open-source models. \textbf{OSWorld}\footnote{\url{https://os-world.github.io/}} (NeurIPS 2024) benchmarks multimodal agents in real computer environments, where recent advances have achieved superhuman performance (76\% vs 72\% human baseline).

\textit{Gap:} Interactive benchmarks evaluate general agent capabilities, not software deployment pipelines.

\subsection{Agent Evaluation Harnesses and Frameworks}

\textbf{Inspect AI}\footnote{\url{https://inspect.aisi.org.uk/}} from the UK AI Safety Institute provides 100+ pre-built evaluations with sandboxing, MCP tool support, and multi-agent primitives. It has been adopted by frontier labs and safety organizations for standardized agent evaluation.

\textbf{DeepEval}\footnote{\url{https://github.com/confident-ai/deepeval}} offers CI/CD integration with LLM-as-judge metrics including task completion, tool correctness, and hallucination detection. \textbf{Databricks Agent Evaluation} integrates with MLflow for tracking groundedness, correctness, and coherence of agentic applications.

The emerging \textbf{AgentOps} paradigm extends DevOps principles to AI agents, addressing observability, tracing, and lifecycle management specific to autonomous systems.

\subsection{DevOps and Deployment Metrics}

\textbf{DORA metrics}~\cite{forsgren2018accelerate}---deployment frequency, lead time, change failure rate, and mean time to restore---provide industry-standard measures of software delivery performance. \textbf{MLOps 2.0} architectures integrate CI/CD with Continuous Data Validation (CDV) for reliable ML delivery.

\textit{Gap:} No existing framework combines code generation evaluation with DORA-mapped deployment metrics and agentic DevX scores (runability, deployability).

\begin{table}[t]
\caption{Comparison of agent evaluation approaches. Klaudbiusz uniquely combines deployment-centric metrics with DORA mapping and trajectory-based optimization.}
\label{tab:related-comparison}
\centering
\small
\begin{tabular}{lccccc}
\toprule
Framework & Code Gen & Deploy & DORA & DevX & Trajectory \\
\midrule
HumanEval/MBPP & \checkmark & -- & -- & -- & -- \\
SWE-bench & \checkmark & -- & -- & -- & -- \\
WebArena/GAIA & -- & -- & -- & -- & -- \\
Inspect AI & \checkmark & -- & -- & -- & -- \\
DeepEval & \checkmark & -- & -- & -- & -- \\
\textbf{Klaudbiusz (Ours)} & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\
\bottomrule
\end{tabular}
\end{table}

%%
%% The Klaudbiusz Framework
%%
\section{The Klaudbiusz Framework}
\label{sec:framework}

\subsection{Design Principles}

Our framework is built on two core principles:

\textbf{Zero-Bias Evaluation.} All metrics are objective, reproducible, and automatable. We explicitly exclude subjective assessments of code quality, maintainability, or aesthetics.

\textbf{Autonomous Deployability.} We measure whether generated applications can be deployed without human intervention, treating deployment capability as a first-class metric: \emph{if an AI agent cannot autonomously run and deploy what it generated, that artifact is not production-ready.}

\subsection{The 13-Metric Rubric}

We organize our metrics into four categories spanning core functionality, platform integration, agentic DevX, and generation efficiency.

\subsubsection{Core Functionality (L1--L4, Binary)}

\begin{itemize}
    \item \textbf{L1: Build Success.} Project compiles; \texttt{docker build} exits with code 0.
    \item \textbf{L2: Runtime Success.} App starts and serves content; health check responds within 30s.
    \item \textbf{L3: Type Safety.} \texttt{npx tsc --noEmit} passes with zero errors.
    \item \textbf{L4: Tests Pass.} Unit/integration tests pass with coverage $\geq$70\%.
\end{itemize}

\subsubsection{Platform Integration (L5--L7, Binary)}

\begin{itemize}
    \item \textbf{L5: DB Connectivity.} Databricks connection works; queries execute without errors.
    \item \textbf{L6: Data Operations.} CRUD operations return correct data from tRPC procedures.
    \item \textbf{L7: UI Validation.} Frontend renders without errors (VLM verification).
\end{itemize}

\subsubsection{Agentic DevX (D8--D9, 0--5 Score)}

\begin{itemize}
    \item \textbf{D8: Runability.} Can a sample AI agent run generated apps locally?
    \begin{itemize}
        \item 0: install/start fails; missing scripts/env
        \item 1--2: starts with manual tweaks
        \item 3: starts cleanly with .env.example + documented steps
        \item 4: starts with seeds/migrations via scripts
        \item 5: + healthcheck endpoint + smoke test succeeds
    \end{itemize}
    \item \textbf{D9: Deployability.} Can a sample AI agent deploy a generated app?
    \begin{itemize}
        \item 0: no/broken Dockerfile
        \item 1--2: image builds; container fails or healthcheck fails
        \item 3: healthcheck OK; smoke 2xx
        \item 4: + logs/metrics hooks present
        \item 5: + automated rollback to prior known-good tag
    \end{itemize}
\end{itemize}

\subsubsection{Efficiency Metrics (E10--E13, Numeric)}

\begin{itemize}
    \item \textbf{E10: Tokens Used.} Total tokens (prompt + completion) for generation.
    \item \textbf{E11: Generation Time.} Time spent generating application (seconds).
    \item \textbf{E12: Agent Turns.} Number of conversation turns during generation.
    \item \textbf{E13: LOC.} Lines of code in generated application.
\end{itemize}

\subsection{AppEval-100 Composite Score}

To enable automatic, comparable measurement across prompts and runs, we introduce \textbf{AppEval-100}---a single numeric index representing normalized readiness and agentic operability on a 0--100 scale.

\textbf{Step 1: Reliability Pillar.} Aggregate runtime-critical checks via geometric mean:
\[
R = \text{GM}(b_{\text{build}}, b_{\text{runtime}}, b_{\text{type}}, b_{\text{tests}}, b_{\text{db}}, \hat{x}_{\text{data}}, \hat{x}_{\text{ui}})
\]

\textbf{Step 2: Agentic DevX Pillar.}
\[
D = \text{GM}(\hat{x}_{\text{run}}, \hat{x}_{\text{deploy}})
\]
where $\hat{x} = \text{score}/5$ normalizes 0--5 scores to $[0,1]$.

\textbf{Step 3: Soft Penalty Gate.} Penalize critical outages without collapsing to zero:
\[
G = (0.25 + 0.75 \times b_{\text{build}}) \times (0.25 + 0.75 \times b_{\text{runtime}}) \times (0.50 + 0.50 \times b_{\text{db}})
\]

\textbf{Step 4: Final Composite.}
\[
\textbf{AppEval-100} = 100 \times (0.7 \times R + 0.3 \times D) \times G
\]

Values near 100 denote near-perfect readiness; 50--70 indicates partial operability; $<$30 signifies fundamental execution issues.

\subsection{DORA Metrics Mapping}

We map our metrics to industry-standard DORA~\cite{forsgren2018accelerate} measures:

\begin{itemize}
    \item \textbf{Deployment Frequency:} Count of successful D9 events per app per evaluation cohort.
    \item \textbf{Lead Time:} Median time from first model call to successful D9 deployment.
    \item \textbf{Change Failure Rate:} Fraction of deployments that fail healthcheck or rollback within 30 min.
    \item \textbf{MTTR:} Median time from failure detection to restore (prior healthy image running).
\end{itemize}

\textbf{Production Gate:} L1--L7 pass, D8$\geq$4, D9$\geq$4, type-safety pass, and DORA guardrails (Lead Time P50 $\leq$10 min, CFR $\leq$15\%, MTTR $\leq$15 min).

\subsection{MLflow Integration}

We integrate with Databricks Managed MLflow for experiment tracking:
\begin{itemize}
    \item Automatic metric logging per evaluation run
    \item Trend analysis across model versions and configurations
    \item Artifact versioning for reproducibility
    \item DORA telemetry for delivery performance monitoring
\end{itemize}

%%
%% Trajectory Optimizer
%%
\section{Trajectory Optimizer}
\label{sec:trajectory}

\subsection{Motivation}

Agent execution trajectories contain rich signal about tool and scaffold failures that cannot be captured by end-state metrics alone. However, manual analysis of trajectories does not scale.

\subsection{Map-Reduce Architecture}

We employ a two-phase analysis approach:

\textbf{Map Phase.} Each trajectory is analyzed independently using a fast model (Claude Haiku). The analysis identifies:
\begin{itemize}
    \item Struggles: errors, retries, confusion patterns
    \item Friction points: slow progress, repeated attempts
    \item Inefficient approaches: suboptimal tool usage
\end{itemize}

\textbf{Reduce Phase.} Individual analyses are synthesized by a more capable model (Claude Opus) with read-only access to the codebase. This phase generates actionable recommendations for:
\begin{itemize}
    \item Template improvements: structure, guidance, scaffolding
    \item Tool improvements: missing tools, unclear descriptions
    \item Root cause analysis: systemic failure patterns
\end{itemize}

\subsection{Feedback Loop}

The trajectory optimizer enables a continuous improvement cycle:

\begin{center}
Generate $\rightarrow$ Evaluate $\rightarrow$ Analyze Trajectories $\rightarrow$ Improve Scaffolding $\rightarrow$ Repeat
\end{center}

%%
%% Experimental Setup
%%
\section{Experimental Setup}
\label{sec:setup}

\subsection{Dataset}

We evaluate on 20 Databricks data application prompts spanning:
\begin{itemize}
    \item Dashboards and analytics interfaces
    \item Data visualization applications
    \item Business intelligence tools
\end{itemize}

\subsection{Generation Pipeline}

Applications are generated using:
\begin{itemize}
    \item \textbf{Claude Agent SDK} with edda MCP for tool orchestration
    \item \textbf{Dagger} containerized execution for isolation and reproducibility
    \item \textbf{Environment scaffolding} with templates and validation pipelines
\end{itemize}

%%
%% Results
%%
\section{Results}
\label{sec:results}

\subsection{Overall Performance}

We evaluate on the ``Simple 20'' prompt set---20 Databricks data application prompts spanning dashboards, analytics, and business intelligence tools.

\begin{table}[h]
\caption{Aggregate evaluation results ($n=20$ applications, Evals 2.0)}
\label{tab:results}
\begin{tabular}{llcc}
\toprule
ID & Metric & Result & Notes \\
\midrule
L1 & Build Success & 20/20 & 100\% pass \\
L2 & Runtime Success & 20/20 & 100\% pass \\
L3 & Type Safety & 1/20 & 5\% pass (improvement needed) \\
L4 & Tests Pass & -- & Not yet instrumented \\
\midrule
L5 & DB Connectivity & 18/20 & 90\% pass \\
L6 & Data Operations & -- & Requires app-specific procedures \\
L7 & UI Validation & -- & VLM check in progress \\
\midrule
D8 & Runability & 3.0/5 & Average score \\
D9 & Deployability & 2.5/5 & Average score \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Finding:} 100\% of generated applications achieve build and runtime success, with 90\% achieving functional Databricks connectivity. However, type safety (5\%) and agentic DevX scores (3.0/5, 2.5/5) indicate room for improvement toward production readiness.

\subsection{Generation Efficiency Metrics}

\begin{table}[h]
\caption{Efficiency metrics ($n=20$ applications)}
\label{tab:efficiency}
\begin{tabular}{lcc}
\toprule
Metric & Value & Notes \\
\midrule
E10: Total Tokens & ~16K/app & Prompt + completion \\
E11: Generation Time & 6--9 min & End-to-end \\
E12: Agent Turns & 93 avg & Conversation turns \\
E13: LOC & 732 avg & Lines of code \\
\midrule
Cost per App & \$0.74 & API cost \\
Total Cost (20 apps) & \$14.81 & -- \\
Build Step Time & 2.7s avg & Docker build \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Comparison: Evals 1.0 vs 2.0}

\begin{table}[h]
\caption{Evolution from manual (Evals 1.0) to automated (Evals 2.0) evaluation}
\label{tab:comparison}
\begin{tabular}{lcc}
\toprule
Aspect & Evals 1.0 & Evals 2.0 \\
\midrule
Viability Rate & 73\% (30 apps) & 100\% build/runtime \\
Time to Deploy & 30--60 min & 6--9 min \\
Evaluation Method & Manual rubric & Automated pipeline \\
Metrics Tracked & Binary viability & 13 metrics + AppEval-100 \\
Reproducibility & Low & Full artifact pack \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Production Readiness Assessment}

Current status: \textbf{below production threshold}. To reach Production Candidate level:
\begin{itemize}
    \item L3 Type Safety: 5\% $\rightarrow$ target $\geq$90\%
    \item D8 Runability: 3.0 $\rightarrow$ target $\geq$4
    \item D9 Deployability: 2.5 $\rightarrow$ target $\geq$4
    \item DORA guardrails: Lead Time P50 $\leq$10m, CFR $\leq$15\%, MTTR $\leq$15m
\end{itemize}

\subsection{Trajectory Optimizer Insights}

Analysis of agent trajectories revealed common friction patterns:
\begin{itemize}
    \item \textbf{SQL Syntax:} Databricks SQL variations causing query failures
    \item \textbf{Error Handling:} Missing error handling in template scaffolding
    \item \textbf{Tool Descriptions:} Unclear MCP tool descriptions leading to incorrect usage
    \item \textbf{Type Inference:} TypeScript strict mode violations in generated code
\end{itemize}

These insights feed back into template and tool improvements via the optimize $\rightarrow$ evaluate $\rightarrow$ analyze cycle.

%%
%% Discussion
%%
\section{Discussion}
\label{sec:discussion}

\subsection{Limitations}

\textbf{Platform Specificity.} Our current implementation targets Databricks applications. Extending to other platforms requires platform-specific metrics (e.g., AWS Lambda, Vercel).

\textbf{Binary Metrics.} Several metrics are binary, potentially missing nuanced quality differences. Future work could introduce continuous variants.

\textbf{Dataset Size.} Our evaluation of 20 applications provides initial validation but may not capture edge cases. Scaling to larger datasets is ongoing.

\subsection{Broader Impact}

By establishing standardized metrics for autonomous deployability, we enable:
\begin{itemize}
    \item Reproducible benchmarking of agentic code generation systems
    \item Objective comparison across different approaches
    \item Systematic improvement through trajectory-based feedback
\end{itemize}

%%
%% Conclusion
%%
\section{Conclusion}
\label{sec:conclusion}

We presented Klaudbiusz, an open-source evaluation framework introducing 9 zero-bias metrics for measuring autonomous deployability of AI-generated applications. Our trajectory optimizer provides actionable feedback for improving agent scaffolding and tools. Evaluated on 20 Databricks applications, we achieve 90\% autonomous deployment readiness.

The path to reliable agentic code generation requires not just better models, but principled evaluation frameworks that measure what matters for production deployment. We release Klaudbiusz to enable the community to benchmark and improve agentic systems systematically.

\textbf{Open Source Release.} Framework, evaluation harness, and trajectory analyzer available at: [URL redacted for review]

%%
%% Acknowledgments (hidden for review)
%%
% \begin{acks}
% Supported by Neon and Databricks.
% \end{acks}

%%
%% Bibliography
%%
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
