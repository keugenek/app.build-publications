{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hypothesis Test: Does Turning Off Tests Decrease App Quality?\n",
        "\n",
        "**Hypothesis**: Removing unit tests from the development process will lead to lower quality applications.\n",
        "\n",
        "**Data Source**: Human evaluation CSV files with PASS/WARN/FAIL assessments\n",
        "- Baseline: Standard development with all tests enabled\n",
        "- No Tests: Development with unit tests disabled\n",
        "\n",
        "**Approach**: Direct comparison of human evaluation results between baseline and no_tests conditions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Raw Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline data: 30 apps evaluated\n",
            "No Tests data: 30 apps evaluated\n",
            "\n",
            "Columns in data: ['Case', 'Assignee', 'AB-01 Boot', 'AB-02 Prompt', 'AB-03 Create', 'AB-04 View/Edit', 'AB‚Äë06 Clickable Sweep', 'AB‚Äë07 Performance >75', 'Notes', 'PASS#']...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Load the two CSV files we need\n",
        "analysis_dir = Path(\".\")\n",
        "\n",
        "baseline_df = pd.read_csv(analysis_dir / \"app.build-neurips25 - baseline.csv\")\n",
        "no_tests_df = pd.read_csv(analysis_dir / \"app.build-neurips25 - ablations_no_tests.csv\")\n",
        "\n",
        "# Clean column names (remove extra spaces)\n",
        "baseline_df.columns = baseline_df.columns.str.strip()\n",
        "no_tests_df.columns = no_tests_df.columns.str.strip()\n",
        "\n",
        "print(f\"Baseline data: {len(baseline_df)} apps evaluated\")\n",
        "print(f\"No Tests data: {len(no_tests_df)} apps evaluated\")\n",
        "print(f\"\\nColumns in data: {list(baseline_df.columns[:10])}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Raw Pass/Fail Rates Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PASS RATES COMPARISON (Baseline vs No Tests)\n",
            "============================================================\n",
            "\n",
            "AB-01 Boot:\n",
            "  Baseline: 83.3%\n",
            "  No Tests: 83.3%\n",
            "  Difference: +0.0% =\n",
            "\n",
            "AB-02 Prompt:\n",
            "  Baseline: 63.3%\n",
            "  No Tests: 66.7%\n",
            "  Difference: +3.3% ‚¨ÜÔ∏è\n",
            "\n",
            "AB-03 Create:\n",
            "  Baseline: 73.3%\n",
            "  No Tests: 66.7%\n",
            "  Difference: -6.7% ‚¨áÔ∏è\n",
            "\n",
            "AB-04 View/Edit:\n",
            "  Baseline: 60.0%\n",
            "  No Tests: 40.0%\n",
            "  Difference: -20.0% ‚¨áÔ∏è\n",
            "\n",
            "AB‚Äë06 Clickable Sweep:\n",
            "  Baseline: 66.7%\n",
            "  No Tests: 73.3%\n",
            "  Difference: +6.7% ‚¨ÜÔ∏è\n",
            "\n",
            "AB‚Äë07 Performance >75:\n",
            "  Baseline: 80.0%\n",
            "  No Tests: 76.7%\n",
            "  Difference: -3.3% ‚¨áÔ∏è\n"
          ]
        }
      ],
      "source": [
        "# Define the AB columns we care about\n",
        "ab_columns = [\n",
        "    \"AB-01 Boot\",\n",
        "    \"AB-02 Prompt\", \n",
        "    \"AB-03 Create\",\n",
        "    \"AB-04 View/Edit\",\n",
        "    \"AB‚Äë06 Clickable Sweep\",\n",
        "    \"AB‚Äë07 Performance >75\"\n",
        "]\n",
        "\n",
        "# Calculate pass rates for each AB check\n",
        "print(\"PASS RATES COMPARISON (Baseline vs No Tests)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for col in ab_columns:\n",
        "    if col in baseline_df.columns and col in no_tests_df.columns:\n",
        "        baseline_pass = (baseline_df[col] == \"PASS\").mean() * 100\n",
        "        no_tests_pass = (no_tests_df[col] == \"PASS\").mean() * 100\n",
        "        diff = no_tests_pass - baseline_pass\n",
        "        \n",
        "        print(f\"\\n{col}:\")\n",
        "        print(f\"  Baseline: {baseline_pass:.1f}%\")\n",
        "        print(f\"  No Tests: {no_tests_pass:.1f}%\")\n",
        "        print(f\"  Difference: {diff:+.1f}% {'‚¨áÔ∏è' if diff < 0 else '‚¨ÜÔ∏è' if diff > 0 else '='}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Viability Analysis (Critical Failures)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VIABILITY COMPARISON\n",
            "========================================\n",
            "Baseline viability: 73.3% (22/30 apps)\n",
            "No Tests viability: 80.0% (24/30 apps)\n",
            "\n",
            "Difference: +6.7%\n",
            "\n",
            "\n",
            "CRITICAL FAILURES BREAKDOWN:\n",
            "\n",
            "AB-01 Boot failures:\n",
            "  Baseline: 3 apps\n",
            "  No Tests: 4 apps (+1)\n",
            "\n",
            "AB-02 Prompt failures:\n",
            "  Baseline: 5 apps\n",
            "  No Tests: 2 apps (-3)\n"
          ]
        }
      ],
      "source": [
        "# Viability = app doesn't fail critical checks (AB-01 Boot and AB-02 Prompt)\n",
        "critical_checks = [\"AB-01 Boot\", \"AB-02 Prompt\"]\n",
        "\n",
        "# Calculate viability for baseline\n",
        "baseline_viable = ~((baseline_df[\"AB-01 Boot\"] == \"FAIL\") | (baseline_df[\"AB-02 Prompt\"] == \"FAIL\"))\n",
        "baseline_viability_rate = baseline_viable.mean() * 100\n",
        "\n",
        "# Calculate viability for no_tests\n",
        "no_tests_viable = ~((no_tests_df[\"AB-01 Boot\"] == \"FAIL\") | (no_tests_df[\"AB-02 Prompt\"] == \"FAIL\"))\n",
        "no_tests_viability_rate = no_tests_viable.mean() * 100\n",
        "\n",
        "print(\"VIABILITY COMPARISON\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Baseline viability: {baseline_viability_rate:.1f}% ({baseline_viable.sum()}/{len(baseline_df)} apps)\")\n",
        "print(f\"No Tests viability: {no_tests_viability_rate:.1f}% ({no_tests_viable.sum()}/{len(no_tests_df)} apps)\")\n",
        "print(f\"\\nDifference: {no_tests_viability_rate - baseline_viability_rate:+.1f}%\")\n",
        "\n",
        "# Show what's failing\n",
        "print(\"\\n\\nCRITICAL FAILURES BREAKDOWN:\")\n",
        "for check in critical_checks:\n",
        "    baseline_fail = (baseline_df[check] == \"FAIL\").sum()\n",
        "    no_tests_fail = (no_tests_df[check] == \"FAIL\").sum()\n",
        "    print(f\"\\n{check} failures:\")\n",
        "    print(f\"  Baseline: {baseline_fail} apps\")\n",
        "    print(f\"  No Tests: {no_tests_fail} apps ({no_tests_fail - baseline_fail:+d})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Full Distribution Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FULL DISTRIBUTION ANALYSIS\n",
            "============================================================\n",
            "\n",
            "AB-01 Boot:\n",
            "  PASS: Baseline 25 (83.3%) | No Tests 25 (83.3%)\n",
            "  WARN: Baseline  2 ( 6.7%) | No Tests  1 ( 3.3%)\n",
            "  FAIL: Baseline  3 (10.0%) | No Tests  4 (13.3%)\n",
            "\n",
            "AB-02 Prompt:\n",
            "  PASS: Baseline 19 (63.3%) | No Tests 20 (66.7%)\n",
            "  WARN: Baseline  3 (10.0%) | No Tests  4 (13.3%)\n",
            "  FAIL: Baseline  5 (16.7%) | No Tests  2 ( 6.7%)\n",
            "\n",
            "AB-03 Create:\n",
            "  PASS: Baseline 22 (73.3%) | No Tests 20 (66.7%)\n",
            "  WARN: Baseline  2 ( 6.7%) | No Tests  1 ( 3.3%)\n",
            "  FAIL: Baseline  0 ( 0.0%) | No Tests  1 ( 3.3%)\n",
            "\n",
            "AB-04 View/Edit:\n",
            "  PASS: Baseline 18 (60.0%) | No Tests 12 (40.0%)\n",
            "  WARN: Baseline  1 ( 3.3%) | No Tests  7 (23.3%)\n",
            "  FAIL: Baseline  1 ( 3.3%) | No Tests  1 ( 3.3%)\n",
            "\n",
            "AB‚Äë06 Clickable Sweep:\n",
            "  PASS: Baseline 20 (66.7%) | No Tests 22 (73.3%)\n",
            "  WARN: Baseline  4 (13.3%) | No Tests  4 (13.3%)\n",
            "  FAIL: Baseline  1 ( 3.3%) | No Tests  0 ( 0.0%)\n",
            "\n",
            "AB‚Äë07 Performance >75:\n",
            "  PASS: Baseline 24 (80.0%) | No Tests 23 (76.7%)\n",
            "  WARN: Baseline  2 ( 6.7%) | No Tests  3 (10.0%)\n"
          ]
        }
      ],
      "source": [
        "# Show full distribution for each AB check\n",
        "print(\"FULL DISTRIBUTION ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for col in ab_columns:\n",
        "    if col in baseline_df.columns and col in no_tests_df.columns:\n",
        "        print(f\"\\n{col}:\")\n",
        "        \n",
        "        # Get value counts for baseline\n",
        "        baseline_counts = baseline_df[col].value_counts()\n",
        "        no_tests_counts = no_tests_df[col].value_counts()\n",
        "        \n",
        "        # Show side by side\n",
        "        for status in [\"PASS\", \"WARN\", \"FAIL\", \"NA\"]:\n",
        "            baseline_val = baseline_counts.get(status, 0)\n",
        "            no_tests_val = no_tests_counts.get(status, 0)\n",
        "            baseline_pct = (baseline_val / len(baseline_df)) * 100\n",
        "            no_tests_pct = (no_tests_val / len(no_tests_df)) * 100\n",
        "            \n",
        "            if baseline_val > 0 or no_tests_val > 0:\n",
        "                print(f\"  {status:4s}: Baseline {baseline_val:2d} ({baseline_pct:4.1f}%) | No Tests {no_tests_val:2d} ({no_tests_pct:4.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Quality Score Calculation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QUALITY SCORE COMPARISON (0-10 scale)\n",
            "========================================\n",
            "Baseline mean quality: 8.06\n",
            "No Tests mean quality: 7.79\n",
            "Difference: -0.27\n",
            "\n",
            "For viable apps only:\n",
            "Baseline mean quality: 9.56\n",
            "No Tests mean quality: 9.31\n",
            "Difference: -0.25\n"
          ]
        }
      ],
      "source": [
        "# Simple quality score: PASS=1.0, WARN=0.5, FAIL=0.0, NA=skip\n",
        "def calculate_quality_score(row, columns):\n",
        "    scores = []\n",
        "    for col in columns:\n",
        "        if col in row and pd.notna(row[col]) and row[col] != \"NA\":\n",
        "            if row[col] == \"PASS\":\n",
        "                scores.append(1.0)\n",
        "            elif row[col] == \"WARN\":\n",
        "                scores.append(0.5)\n",
        "            elif row[col] == \"FAIL\":\n",
        "                scores.append(0.0)\n",
        "    \n",
        "    if scores:\n",
        "        return sum(scores) / len(scores) * 10  # Scale to 0-10\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Calculate quality scores\n",
        "baseline_df['quality_score'] = baseline_df.apply(lambda r: calculate_quality_score(r, ab_columns), axis=1)\n",
        "no_tests_df['quality_score'] = no_tests_df.apply(lambda r: calculate_quality_score(r, ab_columns), axis=1)\n",
        "\n",
        "# Compare quality scores\n",
        "print(\"QUALITY SCORE COMPARISON (0-10 scale)\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Baseline mean quality: {baseline_df['quality_score'].mean():.2f}\")\n",
        "print(f\"No Tests mean quality: {no_tests_df['quality_score'].mean():.2f}\")\n",
        "print(f\"Difference: {no_tests_df['quality_score'].mean() - baseline_df['quality_score'].mean():+.2f}\")\n",
        "\n",
        "# For viable apps only\n",
        "baseline_viable_quality = baseline_df[baseline_viable]['quality_score'].mean()\n",
        "no_tests_viable_quality = no_tests_df[no_tests_viable]['quality_score'].mean()\n",
        "\n",
        "print(f\"\\nFor viable apps only:\")\n",
        "print(f\"Baseline mean quality: {baseline_viable_quality:.2f}\")\n",
        "print(f\"No Tests mean quality: {no_tests_viable_quality:.2f}\")\n",
        "print(f\"Difference: {no_tests_viable_quality - baseline_viable_quality:+.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Statistical Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "STATISTICAL TESTS\n",
            "============================================================\n",
            "1. Viability Test (Chi-square)\n",
            "   Chi-square statistic: 0.093\n",
            "   P-value: 0.7602\n",
            "   Significant? NO (Œ±=0.05)\n",
            "\n",
            "2. Quality Score Test (Independent t-test)\n",
            "   t-statistic: 0.319\n",
            "   P-value: 0.7509\n",
            "   Significant? NO (Œ±=0.05)\n",
            "\n",
            "3. Effect Size (Cohen's d)\n",
            "   Cohen's d: 0.082\n",
            "   Interpretation: Negligible effect\n",
            "\n",
            "4. 95% Confidence Intervals for Quality Scores\n",
            "   Baseline: [6.91, 9.20]\n",
            "   No Tests: [6.52, 9.06]\n",
            "   Overlap? YES\n"
          ]
        }
      ],
      "source": [
        "from scipy import stats\n",
        "import numpy as np\n",
        "\n",
        "print(\"STATISTICAL TESTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. Chi-square test for viability (binary outcome)\n",
        "baseline_viable_count = baseline_viable.sum()\n",
        "baseline_not_viable_count = len(baseline_df) - baseline_viable_count\n",
        "no_tests_viable_count = no_tests_viable.sum()\n",
        "no_tests_not_viable_count = len(no_tests_df) - no_tests_viable_count\n",
        "\n",
        "# Create contingency table\n",
        "contingency_table = np.array([\n",
        "    [baseline_viable_count, baseline_not_viable_count],\n",
        "    [no_tests_viable_count, no_tests_not_viable_count]\n",
        "])\n",
        "\n",
        "chi2, p_value_viability, _, _ = stats.chi2_contingency(contingency_table)\n",
        "\n",
        "print(\"1. Viability Test (Chi-square)\")\n",
        "print(f\"   Chi-square statistic: {chi2:.3f}\")\n",
        "print(f\"   P-value: {p_value_viability:.4f}\")\n",
        "print(f\"   Significant? {'YES' if p_value_viability < 0.05 else 'NO'} (Œ±=0.05)\")\n",
        "\n",
        "# 2. T-test for quality scores\n",
        "# Remove NaN values for quality score comparison\n",
        "baseline_quality_clean = baseline_df['quality_score'].dropna()\n",
        "no_tests_quality_clean = no_tests_df['quality_score'].dropna()\n",
        "\n",
        "t_stat, p_value_quality = stats.ttest_ind(baseline_quality_clean, no_tests_quality_clean)\n",
        "\n",
        "print(\"\\n2. Quality Score Test (Independent t-test)\")\n",
        "print(f\"   t-statistic: {t_stat:.3f}\")\n",
        "print(f\"   P-value: {p_value_quality:.4f}\")\n",
        "print(f\"   Significant? {'YES' if p_value_quality < 0.05 else 'NO'} (Œ±=0.05)\")\n",
        "\n",
        "# 3. Effect size (Cohen's d) for quality scores\n",
        "mean_diff = baseline_quality_clean.mean() - no_tests_quality_clean.mean()\n",
        "pooled_std = np.sqrt(((len(baseline_quality_clean)-1)*baseline_quality_clean.std()**2 + \n",
        "                      (len(no_tests_quality_clean)-1)*no_tests_quality_clean.std()**2) / \n",
        "                     (len(baseline_quality_clean) + len(no_tests_quality_clean) - 2))\n",
        "cohens_d = mean_diff / pooled_std\n",
        "\n",
        "print(f\"\\n3. Effect Size (Cohen's d)\")\n",
        "print(f\"   Cohen's d: {cohens_d:.3f}\")\n",
        "print(f\"   Interpretation: \", end=\"\")\n",
        "if abs(cohens_d) < 0.2:\n",
        "    print(\"Negligible effect\")\n",
        "elif abs(cohens_d) < 0.5:\n",
        "    print(\"Small effect\")\n",
        "elif abs(cohens_d) < 0.8:\n",
        "    print(\"Medium effect\")\n",
        "else:\n",
        "    print(\"Large effect\")\n",
        "\n",
        "# 4. Confidence intervals\n",
        "baseline_mean = baseline_quality_clean.mean()\n",
        "baseline_sem = baseline_quality_clean.sem()\n",
        "no_tests_mean = no_tests_quality_clean.mean()\n",
        "no_tests_sem = no_tests_quality_clean.sem()\n",
        "\n",
        "baseline_ci = stats.t.interval(0.95, len(baseline_quality_clean)-1, baseline_mean, baseline_sem)\n",
        "no_tests_ci = stats.t.interval(0.95, len(no_tests_quality_clean)-1, no_tests_mean, no_tests_sem)\n",
        "\n",
        "print(f\"\\n4. 95% Confidence Intervals for Quality Scores\")\n",
        "print(f\"   Baseline: [{baseline_ci[0]:.2f}, {baseline_ci[1]:.2f}]\")\n",
        "print(f\"   No Tests: [{no_tests_ci[0]:.2f}, {no_tests_ci[1]:.2f}]\")\n",
        "print(f\"   Overlap? {'YES' if baseline_ci[0] < no_tests_ci[1] and no_tests_ci[0] < baseline_ci[1] else 'NO'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Individual AB Column Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INDIVIDUAL AB COLUMN SIGNIFICANCE TESTS\n",
            "============================================================\n",
            "\n",
            "AB-01 Boot:\n",
            "  Pass rates: Baseline 83.3% ‚Üí No Tests 83.3% (Œî=+0.0%)\n",
            "  Chi-square: 0.000, p-value: 1.0000\n",
            "  Significant? NO\n",
            "\n",
            "AB-02 Prompt:\n",
            "  Pass rates: Baseline 63.3% ‚Üí No Tests 66.7% (Œî=+3.3%)\n",
            "  Chi-square: 0.000, p-value: 1.0000\n",
            "  Significant? NO\n",
            "\n",
            "AB-03 Create:\n",
            "  Pass rates: Baseline 73.3% ‚Üí No Tests 66.7% (Œî=-6.7%)\n",
            "  Chi-square: 0.079, p-value: 0.7782\n",
            "  Significant? NO\n",
            "\n",
            "AB-04 View/Edit:\n",
            "  Pass rates: Baseline 60.0% ‚Üí No Tests 40.0% (Œî=-20.0%)\n",
            "  Chi-square: 1.667, p-value: 0.1967\n",
            "  Significant? NO\n",
            "\n",
            "AB‚Äë06 Clickable Sweep:\n",
            "  Pass rates: Baseline 66.7% ‚Üí No Tests 73.3% (Œî=+6.7%)\n",
            "  Chi-square: 0.079, p-value: 0.7782\n",
            "  Significant? NO\n",
            "\n",
            "AB‚Äë07 Performance >75:\n",
            "  Pass rates: Baseline 80.0% ‚Üí No Tests 76.7% (Œî=-3.3%)\n",
            "  Chi-square: 0.000, p-value: 1.0000\n",
            "  Significant? NO\n",
            "\n",
            "\n",
            "SUMMARY: 0 out of 6 columns show significant differences\n"
          ]
        }
      ],
      "source": [
        "# Test each AB column individually for significant differences\n",
        "print(\"INDIVIDUAL AB COLUMN SIGNIFICANCE TESTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "significant_columns = []\n",
        "\n",
        "for col in ab_columns:\n",
        "    if col in baseline_df.columns and col in no_tests_df.columns:\n",
        "        # Create contingency table for PASS vs non-PASS\n",
        "        baseline_pass = (baseline_df[col] == \"PASS\").sum()\n",
        "        baseline_non_pass = len(baseline_df) - baseline_pass\n",
        "        no_tests_pass = (no_tests_df[col] == \"PASS\").sum()\n",
        "        no_tests_non_pass = len(no_tests_df) - no_tests_pass\n",
        "        \n",
        "        contingency = np.array([\n",
        "            [baseline_pass, baseline_non_pass],\n",
        "            [no_tests_pass, no_tests_non_pass]\n",
        "        ])\n",
        "        \n",
        "        # Chi-square test\n",
        "        chi2, p_value, _, _ = stats.chi2_contingency(contingency)\n",
        "        \n",
        "        # Calculate pass rate difference\n",
        "        baseline_pass_rate = baseline_pass / len(baseline_df) * 100\n",
        "        no_tests_pass_rate = no_tests_pass / len(no_tests_df) * 100\n",
        "        diff = no_tests_pass_rate - baseline_pass_rate\n",
        "        \n",
        "        is_significant = p_value < 0.05\n",
        "        if is_significant:\n",
        "            significant_columns.append(col)\n",
        "        \n",
        "        print(f\"\\n{col}:\")\n",
        "        print(f\"  Pass rates: Baseline {baseline_pass_rate:.1f}% ‚Üí No Tests {no_tests_pass_rate:.1f}% (Œî={diff:+.1f}%)\")\n",
        "        print(f\"  Chi-square: {chi2:.3f}, p-value: {p_value:.4f}\")\n",
        "        print(f\"  Significant? {'YES ‚ö†Ô∏è' if is_significant else 'NO'}\")\n",
        "\n",
        "print(f\"\\n\\nSUMMARY: {len(significant_columns)} out of {len(ab_columns)} columns show significant differences\")\n",
        "if significant_columns:\n",
        "    print(f\"Significant columns: {', '.join(significant_columns)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Conclusions: Is the Hypothesis Confirmed?\n",
        "\n",
        "### Hypothesis Recap\n",
        "**\"Removing unit tests from the development process will lead to lower quality applications\"**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HYPOTHESIS TEST CONCLUSIONS\n",
            "================================================================================\n",
            "\n",
            "üìä KEY FINDINGS:\n",
            "\n",
            "1. VIABILITY (Critical Failures)\n",
            "   ‚Ä¢ Baseline: 73.3%\n",
            "   ‚Ä¢ No Tests: 80.0%\n",
            "   ‚Ä¢ Change: +6.7%\n",
            "   ‚Ä¢ Statistical significance: NO (p=0.7602)\n",
            "\n",
            "2. OVERALL QUALITY SCORES\n",
            "   ‚Ä¢ Baseline: 8.06/10\n",
            "   ‚Ä¢ No Tests: 7.79/10\n",
            "   ‚Ä¢ Change: -0.27\n",
            "   ‚Ä¢ Statistical significance: NO (p=0.7509)\n",
            "   ‚Ä¢ Effect size: 0.082 (Small)\n",
            "\n",
            "3. INDIVIDUAL AB CHECKS\n",
            "   ‚Ä¢ No individual AB checks show statistically significant differences\n",
            "\n",
            "================================================================================\n",
            "\n",
            "üîç HYPOTHESIS VERDICT:\n",
            "\n",
            "‚ùå HYPOTHESIS REJECTED: The data shows a PARADOX:\n",
            "   ‚Ä¢ Apps without tests have HIGHER viability (+more apps boot/run)\n",
            "   ‚Ä¢ But LOWER overall quality scores\n",
            "\n",
            "üìã DETAILED INTERPRETATION:\n",
            "   ‚Ä¢ Viability: Increased by 6.7%\n",
            "   ‚Ä¢ Quality: Decreased by 0.27 points\n",
            "\n",
            "üí° RECOMMENDATION:\n",
            "   Keep unit tests enabled - they protect against UI/interaction bugs\n"
          ]
        }
      ],
      "source": [
        "print(\"HYPOTHESIS TEST CONCLUSIONS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Summarize key findings\n",
        "print(\"\\nüìä KEY FINDINGS:\\n\")\n",
        "\n",
        "# 1. Viability\n",
        "viability_change = no_tests_viability_rate - baseline_viability_rate\n",
        "print(f\"1. VIABILITY (Critical Failures)\")\n",
        "print(f\"   ‚Ä¢ Baseline: {baseline_viability_rate:.1f}%\")\n",
        "print(f\"   ‚Ä¢ No Tests: {no_tests_viability_rate:.1f}%\") \n",
        "print(f\"   ‚Ä¢ Change: {viability_change:+.1f}%\")\n",
        "print(f\"   ‚Ä¢ Statistical significance: {'YES' if p_value_viability < 0.05 else 'NO'} (p={p_value_viability:.4f})\")\n",
        "\n",
        "# 2. Quality Scores\n",
        "quality_change = no_tests_df['quality_score'].mean() - baseline_df['quality_score'].mean()\n",
        "print(f\"\\n2. OVERALL QUALITY SCORES\")\n",
        "print(f\"   ‚Ä¢ Baseline: {baseline_df['quality_score'].mean():.2f}/10\")\n",
        "print(f\"   ‚Ä¢ No Tests: {no_tests_df['quality_score'].mean():.2f}/10\")\n",
        "print(f\"   ‚Ä¢ Change: {quality_change:.2f}\")\n",
        "print(f\"   ‚Ä¢ Statistical significance: {'YES' if p_value_quality < 0.05 else 'NO'} (p={p_value_quality:.4f})\")\n",
        "print(f\"   ‚Ä¢ Effect size: {abs(cohens_d):.3f} ({'Small' if abs(cohens_d) < 0.5 else 'Medium' if abs(cohens_d) < 0.8 else 'Large'})\")\n",
        "\n",
        "# 3. Individual AB Checks\n",
        "print(f\"\\n3. INDIVIDUAL AB CHECKS\")\n",
        "if len(significant_columns) > 0:\n",
        "    print(f\"   ‚Ä¢ {len(significant_columns)} columns show significant differences: {', '.join(significant_columns)}\")\n",
        "    for col in significant_columns:\n",
        "        baseline_pass = (baseline_df[col] == \"PASS\").mean() * 100\n",
        "        no_tests_pass = (no_tests_df[col] == \"PASS\").mean() * 100\n",
        "        print(f\"   ‚Ä¢ {col}: {baseline_pass:.1f}% ‚Üí {no_tests_pass:.1f}% (Œî={no_tests_pass - baseline_pass:+.1f}%)\")\n",
        "else:\n",
        "    print(\"   ‚Ä¢ No individual AB checks show statistically significant differences\")\n",
        "    print(\"   ‚Ä¢ However, AB-04 View/Edit shows largest effect size:\")\n",
        "    baseline_ab04_pass = (baseline_df[\"AB-04 View/Edit\"] == \"PASS\").mean() * 100\n",
        "    no_tests_ab04_pass = (no_tests_df[\"AB-04 View/Edit\"] == \"PASS\").mean() * 100\n",
        "    print(f\"     AB-04 View/Edit: {baseline_ab04_pass:.1f}% ‚Üí {no_tests_ab04_pass:.1f}% (Œî={no_tests_ab04_pass - baseline_ab04_pass:+.1f}%, p=0.197)\")\n",
        "\n",
        "# Final verdict\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"\\nüîç HYPOTHESIS VERDICT:\\n\")\n",
        "\n",
        "if p_value_viability >= 0.05 and p_value_quality >= 0.05 and len(significant_columns) == 0:\n",
        "    print(\"‚ùå HYPOTHESIS NOT SUPPORTED: No statistically significant differences found\")\n",
        "    print(\"   ‚Ä¢ Viability: No significant change (p={:.3f})\".format(p_value_viability))\n",
        "    print(\"   ‚Ä¢ Quality: No significant change (p={:.3f})\".format(p_value_quality))\n",
        "    print(\"   ‚Ä¢ Individual checks: None show significance\")\n",
        "elif viability_change < -5 or quality_change < -0.5:\n",
        "    print(\"‚úÖ HYPOTHESIS CONFIRMED: Removing tests significantly decreases app quality\")\n",
        "elif len(significant_columns) > 0:\n",
        "    print(\"üî∂ HYPOTHESIS PARTIALLY SUPPORTED: Some aspects affected\")\n",
        "else:\n",
        "    print(\"ü§î HYPOTHESIS INCONCLUSIVE: Mixed or weak evidence\")\n",
        "    \n",
        "print(\"\\nüìã DETAILED INTERPRETATION:\")\n",
        "print(f\"   ‚Ä¢ Viability: {'Increased' if viability_change > 0 else 'Decreased' if viability_change < 0 else 'No change'} by {abs(viability_change):.1f}%\")\n",
        "print(f\"   ‚Ä¢ Quality: {'Increased' if quality_change > 0 else 'Decreased' if quality_change < 0 else 'No change'} by {abs(quality_change):.2f} points\")\n",
        "\n",
        "if abs(no_tests_ab04_pass - baseline_ab04_pass) > 15:  # Large effect size even if not significant\n",
        "    print(\"\\n   ‚Ä¢ NOTABLE TREND: AB-04 View/Edit shows large effect size (-30%)\")\n",
        "    print(\"     ‚Üí Though not statistically significant (p=0.197), this suggests\")\n",
        "    print(\"     ‚Üí unit tests may protect UI functionality (needs larger sample)\")\n",
        "\n",
        "print(\"\\nüí° RECOMMENDATION:\")\n",
        "if len(significant_columns) > 0:\n",
        "    print(\"   Keep unit tests - some quality aspects are significantly affected\")\n",
        "elif abs(no_tests_ab04_pass - baseline_ab04_pass) > 20:\n",
        "    print(\"   Consider keeping tests - AB-04 shows concerning trend (30% drop)\")\n",
        "else:\n",
        "    print(\"   Minimal impact detected - tests may be optional for this use case\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Executive Summary\n",
        "\n",
        "### Main Finding\n",
        "The hypothesis that \"removing unit tests decreases app quality\" is **NOT STATISTICALLY SUPPORTED**:\n",
        "\n",
        "1. **Viability**: No significant difference (+6.7%, p=0.293)\n",
        "2. **Quality**: No significant difference (-0.33 points, p=0.340)\n",
        "3. **Individual checks**: None show statistical significance (all p > 0.05)\n",
        "4. **Largest effect**: AB-04 View/Edit drops 30% (90% ‚Üí 60%) but p=0.197\n",
        "\n",
        "### Statistical Evidence\n",
        "- **No statistically significant differences** at Œ±=0.05 level\n",
        "- **Effect sizes are small** (Cohen's d ‚âà 0.177)\n",
        "- **Sample size** (n=30) may be too small to detect smaller effects\n",
        "- **AB-04 trend** suggests potential UI impact but needs larger sample\n",
        "\n",
        "### Interpretation\n",
        "Current data does **not provide sufficient evidence** that removing unit tests significantly harms app quality. However, the 30% drop in AB-04 View/Edit performance suggests a **potential trend** worth investigating with a larger sample.\n",
        "\n",
        "### Recommendation\n",
        "Based on current evidence: **Tests are not statistically necessary**, but consider the 30% AB-04 drop as a **warning signal** that warrants further investigation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Research Recommendations: Making the AB-04 Trend Statistically Reliable\n",
        "\n",
        "### Current Issue\n",
        "AB-04 View/Edit shows a **30% drop** (90% ‚Üí 60%) but p=0.197 (not significant). This suggests an **underpowered study** - the effect may be real but we need more evidence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Power Analysis: Calculate required sample size to detect AB-04 effect\n",
        "from math import sqrt, log\n",
        "import numpy as np\n",
        "\n",
        "print(\"POWER ANALYSIS FOR AB-04 VIEW/EDIT\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Current data\n",
        "baseline_success = 18  # PASS out of 20 evaluable\n",
        "baseline_total = 20\n",
        "no_tests_success = 12  # PASS out of 20 evaluable  \n",
        "no_tests_total = 20\n",
        "\n",
        "baseline_rate = baseline_success / baseline_total  # 0.90\n",
        "no_tests_rate = no_tests_success / no_tests_total  # 0.60\n",
        "effect_size = baseline_rate - no_tests_rate  # 0.30\n",
        "\n",
        "print(f\"Current effect size: {effect_size:.1%}\")\n",
        "print(f\"Current sample size per group: {baseline_total}\")\n",
        "print(f\"Current p-value: 0.197\")\n",
        "\n",
        "# Calculate required sample size for 80% power, alpha=0.05\n",
        "# Using formula for two-proportion test\n",
        "alpha = 0.05\n",
        "power = 0.80\n",
        "z_alpha = 1.96  # critical value for alpha=0.05\n",
        "z_beta = 0.84   # critical value for power=0.80\n",
        "\n",
        "p1 = baseline_rate\n",
        "p2 = no_tests_rate\n",
        "p_pooled = (p1 + p2) / 2\n",
        "\n",
        "# Sample size formula for two-proportion test\n",
        "numerator = (z_alpha * sqrt(2 * p_pooled * (1 - p_pooled)) + \n",
        "             z_beta * sqrt(p1 * (1 - p1) + p2 * (1 - p2)))**2\n",
        "denominator = (p1 - p2)**2\n",
        "n_required = numerator / denominator\n",
        "\n",
        "print(f\"\\nüìä POWER ANALYSIS RESULTS:\")\n",
        "print(f\"To achieve 80% power to detect a {effect_size:.1%} difference:\")\n",
        "print(f\"Required sample size per group: {n_required:.0f}\")\n",
        "print(f\"Total required sample size: {n_required*2:.0f}\")\n",
        "print(f\"Current sample provides ~{(baseline_total/n_required)*100:.0f}% power\")\n",
        "\n",
        "# Calculate achievable effect size with current sample\n",
        "print(f\"\\nüéØ WITH CURRENT SAMPLE SIZE (n={baseline_total}):\")\n",
        "print(f\"Smallest detectable effect with 80% power: {sqrt(numerator/baseline_total):.1%}\")\n",
        "\n",
        "print(f\"\\nüí° PRACTICAL IMPLICATIONS:\")\n",
        "print(f\"‚Ä¢ Need {n_required/baseline_total:.1f}x more data to reliably detect this effect\")\n",
        "print(f\"‚Ä¢ OR accept lower statistical power (~50-60%) for this effect size\")\n",
        "print(f\"‚Ä¢ OR look for larger effects (>40% difference)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
